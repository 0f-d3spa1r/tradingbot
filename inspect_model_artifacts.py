# inspect_model_artifacts.py
import argparse
import json
import logging
import os
from pathlib import Path

import pickle
import numpy as np

try:
    import catboost as cb
except ImportError:
    cb = None

logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

PROJECT_ROOT = Path(__file__).resolve().parent
MODEL_DIR = PROJECT_ROOT / "models"


def _safe_load_pickle(path: Path, label: str):
    if not path.exists():
        logger.warning(f"[{label}] —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")
        return None
    try:
        with open(path, "rb") as f:
            obj = pickle.load(f)
        logger.info(f"[{label}] –∑–∞–≥—Ä—É–∂–µ–Ω: {path}")
        return obj
    except Exception as e:
        logger.warning(f"[{label}] –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {path}: {e}")
        return None


def _safe_load_json(path: Path, label: str):
    if not path.exists():
        logger.warning(f"[{label}] —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")
        return None
    try:
        with open(path, "r", encoding="utf-8") as f:
            obj = json.load(f)
        logger.info(f"[{label}] –∑–∞–≥—Ä—É–∂–µ–Ω: {path}")
        return obj
    except Exception as e:
        logger.warning(f"[{label}] –æ—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {path}: {e}")
        return None


def _find_latest_run_dir(symbol: str) -> Path | None:
    base = MODEL_DIR / symbol
    if not base.exists() or not base.is_dir():
        logger.warning(f"–î–ª—è —Å–∏–º–≤–æ–ª–∞ {symbol} –Ω–µ—Ç –ø–∞–ø–∫–∏ {base}")
        return None

    candidates = [p for p in base.iterdir() if p.is_dir()]
    if not candidates:
        logger.warning(f"–î–ª—è —Å–∏–º–≤–æ–ª–∞ {symbol} –Ω–µ—Ç –Ω–∏ –æ–¥–Ω–æ–≥–æ run_dir –≤ {base}")
        return None

    # —Å–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –∏–º–µ–Ω–∏ (ts –≤ —Ñ–æ—Ä–º–∞—Ç–µ YYYYMMDD_HHMMSS ‚Üí –ª–µ–∫—Å–∏–∫–æ–≥—Ä–∞—Ñ–∏—á–µ—Å–∫–∏ –æ–∫)
    latest = sorted(candidates, key=lambda p: p.name)[-1]
    logger.info(f"–ù–∞–π–¥–µ–Ω –ø–æ—Å–ª–µ–¥–Ω–∏–π run_dir –¥–ª—è {symbol}: {latest}")
    return latest


def inspect_run(symbol: str, ts: str | None):
    """
    –ò–Ω—Å–ø–µ–∫—Ç–∏—Ä—É–µ—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —É—Ä–æ–≤–Ω—è run_dir: models/<symbol>/<ts>/
    –ï—Å–ª–∏ ts=None, –±–µ—Ä—ë—Ç –ø–æ—Å–ª–µ–¥–Ω–∏–π –ø–æ –∏–º–µ–Ω–∏.
    """
    if ts is None:
        run_dir = _find_latest_run_dir(symbol)
        if run_dir is None:
            logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ run_dir ‚Äî –≤—ã—Ö–æ–¥–∏–º.")
            return
    else:
        run_dir = MODEL_DIR / symbol / ts
        if not run_dir.exists():
            logger.error(f"–£–∫–∞–∑–∞–Ω–Ω—ã–π run_dir –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç: {run_dir}")
            return
        logger.info(f"–ò—Å–ø–æ–ª—å–∑—É—é run_dir: {run_dir}")

    model_path = run_dir / "model.cbm"
    feat_path = run_dir / "feature_columns.pkl"
    cat_path = run_dir / "cat_features.pkl"
    scaler_path = run_dir / "scaler.pkl"
    temp_path = run_dir / "temperature.json"
    calib_path = run_dir / "confidence_calibrator.pkl"

    print("\n" + "=" * 80)
    print(f"üß© ARTEFACTS FOR SYMBOL={symbol}, RUN={run_dir.name}")
    print("=" * 80 + "\n")

    # --- feature_columns ---
    feature_columns = _safe_load_pickle(feat_path, "feature_columns")
    if feature_columns is not None:
        print(f"feature_columns.pkl ‚Üí type={type(feature_columns)}, len={len(feature_columns)}")
        if isinstance(feature_columns, list):
            preview = feature_columns[:20]
            print(f"  first 20 features: {preview}")
        print()

    # --- cat_features ---
    cat_features = _safe_load_pickle(cat_path, "cat_features")
    if cat_features is not None:
        print(f"cat_features.pkl ‚Üí type={type(cat_features)}, len={len(cat_features)}")
        if isinstance(cat_features, list):
            preview = cat_features[:20]
            print(f"  cat feature names: {preview}")
        print()

    # --- scaler ---
    scaler = _safe_load_pickle(scaler_path, "scaler")
    if scaler is not None:
        print(f"scaler.pkl ‚Üí type={type(scaler)}")
        if hasattr(scaler, "feature_names_in_"):
            names = list(scaler.feature_names_in_)
            print(f"  feature_names_in_ (len={len(names)}): {names}")
        else:
            print("  ‚ö† scaler has no feature_names_in_")
        print()

    # --- temperature ---
    temp_json = _safe_load_json(temp_path, "temperature")
    if temp_json is not None:
        T = temp_json.get("T", None)
        print(f"temperature.json ‚Üí {temp_json}")
        print(f"  parsed T = {T}")
        print()

    # --- calibrator ---
    calibrator = _safe_load_pickle(calib_path, "confidence_calibrator")
    if calibrator is not None:
        print(f"confidence_calibrator.pkl ‚Üí type={type(calibrator)}")
        # –Ω–µ–±–æ–ª—å—à–∞—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞
        attrs = [a for a in dir(calibrator) if a.endswith("_min_") or a.endswith("_max_")]
        if attrs:
            print(f"  attrs: {attrs}")
        print()

    # --- model.cbm ---
    if not model_path.exists():
        logger.warning(f"[model] —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
    else:
        print(f"model.cbm ‚Üí {model_path}")
        if cb is None:
            print("  ‚ö† catboost –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ —ç—Ç–æ–º –æ–∫—Ä—É–∂–µ–Ω–∏–∏ ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é –∑–∞–≥—Ä—É–∑–∫—É –º–æ–¥–µ–ª–∏.")
        else:
            try:
                model = cb.CatBoostClassifier()
                model.load_model(str(model_path))
                params = model.get_params()
                print("  model params (subset):")
                for k in ["iterations", "depth", "learning_rate", "loss_function"]:
                    if k in params:
                        print(f"    {k}: {params[k]}")
                # shape –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–µ–π, –µ—Å–ª–∏ —Ö–æ—Ç–∏–º
                try:
                    # —Å–¥–µ–ª–∞–µ–º —Ñ–µ–π–∫–æ–≤—ã–π input –∏–∑ –æ–¥–Ω–æ–≥–æ –Ω—É–ª–µ–≤–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞
                    if feature_columns and isinstance(feature_columns, list):
                        import pandas as pd
                        X_dummy = pd.DataFrame([np.zeros(len(feature_columns))], columns=feature_columns)
                        proba = model.predict_proba(X_dummy)
                        print(f"  predict_proba(dummy) shape: {np.asarray(proba).shape}")
                except Exception as e:
                    print(f"  (diagnostic predict_proba failed: {e})")
            except Exception as e:
                print(f"  ‚ö† –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        print()

    print("=" * 80)
    print("‚úÖ –ò–Ω—Å–ø–µ–∫—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞.")
    print("=" * 80)


def main():
    parser = argparse.ArgumentParser(
        description="–ò–Ω—Å–ø–µ–∫—Ü–∏—è –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ –º–æ–¥–µ–ª–∏ (feature_columns, cat_features, scaler, T, calibrator, model)."
    )
    parser.add_argument(
        "--symbol",
        type=str,
        required=True,
        help="–¢–∏–∫–µ—Ä/—Å–∏–º–≤–æ–ª, –Ω–∞–ø—Ä–∏–º–µ—Ä BTCUSDT –∏–ª–∏ ETHUSDT",
    )
    parser.add_argument(
        "--ts",
        type=str,
        default=None,
        help="–¢–∞–π–º—Å—Ç–µ–º–ø –ø—Ä–æ–≥–æ–Ω–∞ (–ø–æ–¥–ø–∞–ø–∫–∞ –≤ models/<symbol>/). –ï—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω ‚Äî –±–µ—Ä—ë—Ç—Å—è –ø–æ—Å–ª–µ–¥–Ω–∏–π.",
    )

    args = parser.parse_args()
    inspect_run(symbol=args.symbol, ts=args.ts)


if __name__ == "__main__":
    main()

#python inspect_model_artifacts.py --symbol ETHUSDT
#python inspect_model_artifacts.py --symbol BTCUSDT