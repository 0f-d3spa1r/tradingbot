# pipeline.py
import os
import pandas as pd
import shutil
import pickle
import logging
from datetime import datetime, timezone
from pathlib import Path

import numpy as np
import matplotlib.pyplot as plt
import catboost as cb
from sklearn.metrics import (
    accuracy_score, classification_report, f1_score,
    ConfusionMatrixDisplay
)

from pybit.unified_trading import HTTP
from config import BYBIT_API_KEY, BYBIT_API_SECRET, CONFIDENCE_THRESHOLDS
from data_loader import set_client

from model_trainer import (
    prepare_data,
    optimize_catboost,
    train_final_model,
    rolling_cross_validation,
)
from confidence_calibrator import fit_confidence_calibrator, save_calibrator
import json
from datetime import datetime
from config import EMBARGO_BARS, MIN_CV_TRAIN, MIN_CV_VAL



# --- setup ---
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

OUTPUT_DIR = Path("outputs")
MODEL_DIR = Path("models")
OUTPUT_DIR.mkdir(exist_ok=True)
MODEL_DIR.mkdir(exist_ok=True)


# === Align helper: ensures test/holdout columns match training features ===
import pickle
import catboost as cb

def _align_for_infer(df: pd.DataFrame) -> tuple[pd.DataFrame, list[int]]:
    """
    –ü—Ä–∏–≤–æ–¥–∏—Ç –≤—Ö–æ–¥–Ω–æ–π DF –∫ —Ç–æ–º—É –∂–µ –Ω–∞–±–æ—Ä—É/–ø–æ—Ä—è–¥–∫—É –ø—Ä–∏–∑–Ω–∞–∫–æ–≤, —á—Ç–æ –∏ –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏:
    - –ø—Ä–æ–±—É–µ—Ç –∑–∞–≥—Ä—É–∑–∏—Ç—å models/feature_columns.pkl (–µ—Å–ª–∏ –¥–µ–ª–∞–ª—Å—è feature bagging)
    - —Ñ–∏–ª—å—Ç—Ä—É–µ—Ç/–ø–µ—Ä–µ—É–ø–æ—Ä—è–¥–æ—á–∏–≤–∞–µ—Ç —Å—Ç–æ–ª–±—Ü—ã
    - —Å–∞–Ω–∏—Ç–∏–∑–∏—Ä—É–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ (string + fillna("__NA__"))
    - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç df_aligned –∏ –∏–Ω–¥–µ–∫—Å—ã cat_features –¥–ª—è CatBoost Pool
    """
    keep_cols = None
    try:
        with open("models/feature_columns.pkl", "rb") as f:
            keep_cols = pickle.load(f)
    except Exception:
        keep_cols = df.columns.tolist()
        logger.warning("[InferAlign] models/feature_columns.pkl not found ‚Äî using current columns order")

    # filter + reorder
    cols = [c for c in keep_cols if c in df.columns]
    if len(cols) < len(keep_cols):
        missing = [c for c in keep_cols if c not in df.columns]
        logger.warning("[InferAlign] %d feature(s) missing in input: %s", len(missing), missing[:10])

    df2 = df[cols].copy()

    # sanitize categoricals
    cat_cols = df2.select_dtypes(include=["object", "category", "string"]).columns.tolist()
    if cat_cols:
        df2[cat_cols] = df2[cat_cols].astype("string").fillna("__NA__")

    cat_idx = [df2.columns.get_loc(c) for c in cat_cols]
    logger.info("[InferAlign] kept %d features; cat=%d", len(cols), len(cat_idx))
    return df2, cat_idx



def _apply_temperature_scaling(proba: np.ndarray, T: float) -> np.ndarray:
    """Softmax temperature scaling for multi-class, from probabilities (logit-free)."""
    logits = np.log(np.clip(proba, 1e-12, 1.0))
    logits_T = logits / max(T, 1e-6)
    m = np.max(logits_T, axis=1, keepdims=True)
    exp = np.exp(logits_T - m)
    return exp / np.sum(exp, axis=1, keepdims=True)

def _nll_multiclass(y_true: np.ndarray, proba: np.ndarray) -> float:
    p = np.clip(proba[np.arange(len(y_true)), y_true], 1e-12, 1.0)
    return float(-np.mean(np.log(p)))

def _find_best_temperature(proba_val: np.ndarray, y_val: np.ndarray,
                           t_min: float, t_max: float, t_step: float) -> float:
    best_T, best_nll = 1.0, _nll_multiclass(y_val, proba_val)
    T = t_min
    while T <= t_max + 1e-9:
        proba_T = _apply_temperature_scaling(proba_val, T)
        nll = _nll_multiclass(y_val, proba_T)
        if nll < best_nll:
            best_T, best_nll = float(T), float(nll)
        T += t_step
    return best_T


def find_threshold_for_precision(y_true, proba, target_precision=0.6):
    """
    –ü–æ–¥–±–∏—Ä–∞–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º precision >= target_precision.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (threshold, coverage).
    """
    from sklearn.metrics import precision_score

    conf = proba.max(axis=1)
    y_pred = proba.argmax(axis=1)
    for th in np.linspace(0.9, 0.3, 13):  # —à–∞–≥ 0.05 –≤–Ω–∏–∑ (0.9 ‚Üí 0.3)
        m = conf >= th
        if not m.any():
            continue
        prec = precision_score(y_true[m], y_pred[m], average="macro", zero_division=0)
        if prec >= target_precision:
            return th, m.mean()  # –Ω–∞–π–¥–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –∏ –¥–æ–ª—è –æ—Ö–≤–∞—Ç–∞
    return None, 0.0


# ===== Inference alignment helpers =====
def _load_feature_and_cat_lists(symbol: str | None = None):
    """
    –ì—Ä—É–∑–∏–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –∏ –∫–∞—Ç-—Ñ–∏—á–µ–π, —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏.
    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: models/<SYMBOL>/<ts>/feature_columns.pkl (–µ—Å–ª–∏ —Ö–æ—á–µ—à—å ‚Äî –¥–æ–±–∞–≤—å),
               models/feature_columns.pkl,
               models/feature_columns_<SYMBOL>.pkl (–µ—Å–ª–∏ –≤–µ–¥—ë—à—å –ø–æ-—Å–∏–º–≤–æ–ª—å–Ω–æ).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (feature_columns: list[str] | None, cat_features: list[str] | None)
    """
    candidates = []
    if symbol:
        candidates += [
            MODEL_DIR / f"feature_columns_{symbol}.pkl",
        ]
    candidates += [
        MODEL_DIR / "feature_columns.pkl",
    ]
    feat_cols = None
    for p in candidates:
        if p.exists():
            with open(p, "rb") as f:
                feat_cols = pickle.load(f)
            break

    # cat_features
    cat_list = None
    cat_paths = []
    if symbol:
        cat_paths += [MODEL_DIR / f"cat_features_{symbol}.pkl"]
    cat_paths += [MODEL_DIR / "cat_features.pkl"]
    for p in cat_paths:
        if p.exists():
            with open(p, "rb") as f:
                cat_list = pickle.load(f)
            break

    return feat_cols, cat_list


def _align_for_infer(X: pd.DataFrame, symbol: str | None = None):
    """
    –î–µ–ª–∞–µ—Ç –≤—Ö–æ–¥–Ω–æ–π X —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–º —Å –º–æ–¥–µ–ª—å—é:
    - –≤—ã–±–∏—Ä–∞–µ—Ç –¢–ï –ñ–ï –∫–æ–ª–æ–Ω–∫–∏ –≤ –¢–û–ú –ñ–ï –ø–æ—Ä—è–¥–∫–µ, —á—Ç–æ –∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏;
    - –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ (0.0 –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö, '__NA__' –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö);
    - –ø—Ä–∏–≤–æ–¥–∏—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫ pandas StringDtype;
    - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (X_aligned, cat_idx).
    """
    X_in = X.copy()

    # 1) –≥—Ä—É–∑–∏–º —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
    feat_cols, cat_features = _load_feature_and_cat_lists(symbol)

    # –µ—Å–ª–∏ —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ, —á—Ç–æ –ø—Ä–∏—à–ª–æ (–Ω–æ —ç—Ç–æ —Ä–∏—Å–∫!)
    if feat_cols is None:
        feat_cols = X_in.columns.tolist()

    # 2) –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    #    –æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–∏–µ ‚Äî —Å–æ–∑–¥–∞—ë–º (—á–∏—Å–ª–æ–≤—ã–µ -> 0.0; –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ -> "__NA__")
    exist_cols = set(X_in.columns)
    need_cols = list(feat_cols)

    # –µ—Å–ª–∏ —Å–ø–∏—Å–æ–∫ cat_features –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –æ–ø—Ä–µ–¥–µ–ª–∏–º –ø–æ dtype –≤—Ö–æ–¥–Ω–æ–≥–æ X (best-effort)
    if cat_features is None:
        cat_features = X_in.select_dtypes(include=["object", "category", "string"]).columns.tolist()

    cat_set = set(cat_features)

    for col in need_cols:
        if col not in exist_cols:
            if col in cat_set:
                X_in[col] = "__NA__"
            else:
                X_in[col] = 0.0

    # 3) —Ä–µ–∂–µ–º –ª–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –∏ —Å—Ç–∞–≤–∏–º –ü–†–ê–í–ò–õ–¨–ù–´–ô –ü–û–†–Ø–î–û–ö
    X_in = X_in[need_cols]

    # 4) —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö
    cat_cols_present = [c for c in cat_features if c in X_in.columns]
    if cat_cols_present:
        X_in[cat_cols_present] = X_in[cat_cols_present].astype("string").fillna("__NA__")

    # 5) –∏–Ω–¥–µ–∫—Å—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö (CatBoost –æ–∂–∏–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å—ã/–ø–æ–∑–∏—Ü–∏–∏)
    cat_idx = [X_in.columns.get_loc(c) for c in cat_cols_present]

    return X_in, cat_idx

def evaluate_model(model, X_test, y_test, symbol="model", ts=None, calib=None):
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ —Ç–µ—Å—Ç–µ (eval) –∏, –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω calib=(X_hold, y_hold),
    –ø–æ–¥–±–∏—Ä–∞–µ—Ç Temperature T –Ω–∞ holdout (–ø–æ NLL), –ø—Ä–∏–º–µ–Ω—è–µ—Ç –µ–≥–æ, –æ–±—É—á–∞–µ—Ç –∏–∑–æ—Ç–æ–Ω–∏—á–µ—Å–∫–∏–π
    –∫–∞–ª–∏–±—Ä–∞—Ç–æ—Ä —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ holdout –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏ –∫–∞–ª–∏–±—Ä–∞—Ç–æ—Ä, –∏ T.
    """
    from config import (
        CONFIDENCE_THRESHOLDS,
        TEMPERATURE_SCALING, TEMPERATURE_MIN, TEMPERATURE_MAX, TEMPERATURE_STEP,
    )

    tag = f"{symbol}" + (f"_{ts}" if ts else "")
    run_dir = MODEL_DIR / symbol / (ts if ts else "")
    run_dir.mkdir(parents=True, exist_ok=True)

    # --- –ª–æ–∫–∞–ª—å–Ω—ã–π —Ö–µ–ª–ø–µ—Ä –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    def _log_conf_stats(name: str, proba: np.ndarray):
        conf = proba.max(axis=1)
        logger.info("[%s] mean_conf=%.3f | p90=%.3f | p95=%.3f | max=%.3f",
                    name,
                    float(conf.mean()),
                    float(np.quantile(conf, 0.90)),
                    float(np.quantile(conf, 0.95)),
                    float(conf.max()))

    # =========================
    # 1) Temperature: –ø–æ–¥–æ–±—Ä–∞—Ç—å –ø–æ HOLDOUT (–µ—Å–ª–∏ –µ—Å—Ç—å)
    # =========================
    best_T = 1.0
    proba_hold = None
    y_hold_np = None

    if calib is not None:
        X_hold, y_hold = calib

        # 1.1 –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ HOLDOUT –ø–æ–¥ –æ–±—É—á–∞—é—â–∏–π –ø–∞–π–ø–ª–∞–π–Ω (–ø–æ—Ä—è–¥–æ–∫ —Ñ–∏—á, cat idx)
        X_hold_aligned, cat_idx_hold = _align_for_infer(X_hold)
        pool_hold = cb.Pool(X_hold_aligned, cat_features=cat_idx_hold)

        # 1.2 –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ holdout –¥–æ T
        proba_hold_raw = np.asarray(model.predict_proba(pool_hold))
        y_hold_np = np.asarray(y_hold).astype(int)

        _log_conf_stats("HOLD raw", proba_hold_raw)

        # 1.3 –ü–æ–¥–±–æ—Ä T –ø–æ NLL (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –∏ –¥–∞–Ω–Ω—ã—Ö –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ)
        if TEMPERATURE_SCALING and len(y_hold_np) >= 30:
            # –æ–≥—Ä–∞–Ω–∏—á–∏–º –º–∞–∫—Å–∏–º—É–º T (–µ—Å–ª–∏ –≤ –∫–æ–Ω—Ñ–∏–≥–µ —Ç–∞–∫ –∑–∞–¥–∞–Ω–æ)
            T_min = float(TEMPERATURE_MIN)
            T_max = float(TEMPERATURE_MAX)
            T_step = float(TEMPERATURE_STEP)

            best_T = _find_best_temperature(proba_hold_raw, y_hold_np, T_min, T_max, T_step)
            logger.info("Temperature scaling: candidate T=%.2f (chosen on holdout by NLL)", best_T)

        # 1.4 –ü—Ä–∏–º–µ–Ω—è–µ–º T –∏ –¥–µ–ª–∞–µ–º ¬´safeguard¬ª: –µ—Å–ª–∏ T —É—Ö—É–¥—à–∏–ª macro-F1 –Ω–∞ holdout ‚Äî –æ—Ç–∫–∞—Ç—ã–≤–∞–µ–º—Å—è
        y_pred_hold_raw = proba_hold_raw.argmax(axis=1).astype(int)
        f1m_raw = f1_score(y_hold_np, y_pred_hold_raw, average="macro")

        proba_hold_T = _apply_temperature_scaling(proba_hold_raw, best_T)
        y_pred_hold_T = proba_hold_T.argmax(axis=1).astype(int)
        f1m_T = f1_score(y_hold_np, y_pred_hold_T, average="macro")

        if f1m_T + 1e-9 < f1m_raw - 0.01:  # –¥–æ–ø—É—Å–∫–∞–µ–º –Ω–µ–∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ–µ –∫–æ–ª–µ–±–∞–Ω–∏–µ
            logger.info("Temperature rollback: F1 holdout decreased (raw=%.4f -> T=%.4f). Using T=1.0",
                        f1m_raw, f1m_T)
            best_T = 1.0
            proba_hold = proba_hold_raw
        else:
            proba_hold = proba_hold_T

        _log_conf_stats("HOLD T", proba_hold)

        # 1.5 –°–æ—Ö—Ä–∞–Ω—è–µ–º T (–∏ –∞–ª–∏–∞—Å—ã)
        try:
            (run_dir / "temperature.json").write_text(json.dumps({"T": best_T}))
            (MODEL_DIR / f"temperature_{symbol}.json").write_text(json.dumps({"T": best_T}))
            (MODEL_DIR / "temperature.json").write_text(json.dumps({"T": best_T}))
        except Exception as e:
            logger.warning("Failed to save temperature.json: %s", e)

    # =========================
    # 2) –û—Ü–µ–Ω–∫–∞ –Ω–∞ TEST (eval), —É–∂–µ —Å –ø—Ä–∏–º–µ–Ω—ë–Ω–Ω—ã–º T
    # =========================
    # 2.1 –í—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ TEST –ø–æ–¥ –æ–±—É—á–∞—é—â–∏–π –ø–∞–π–ø–ª–∞–π–Ω
    X_test_aligned, cat_idx_test = _align_for_infer(X_test)
    pool_test = cb.Pool(X_test_aligned, cat_features=cat_idx_test)

    # 2.2 –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ eval + temperature
    proba_eval = np.asarray(model.predict_proba(pool_test))
    proba_eval = _apply_temperature_scaling(proba_eval, best_T)
    _log_conf_stats("EVAL T", proba_eval)

    y_test_np   = np.asarray(y_test).astype(int)
    y_pred_eval = proba_eval.argmax(axis=1).astype(int)
    conf_eval   = proba_eval.max(axis=1)

    logger.info("=" * 30 + f" [FINAL METRICS] {tag} " + "=" * 30)
    acc_eval = accuracy_score(y_test_np, y_pred_eval)
    f1m_eval = f1_score(y_test_np, y_pred_eval, average='macro')
    logger.info("[All] Accuracy: %.4f", acc_eval)
    logger.info("[All] F1 macro: %.4f", f1m_eval)

    # per-class F1 (–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞)
    try:
        from sklearn.metrics import precision_recall_fscore_support
        _, _, f1_per_cls, _ = precision_recall_fscore_support(
            y_test_np, y_pred_eval, labels=[0, 1, 2], zero_division=0
        )
        logger.info("[Eval per-class] F1: Down=%.3f | Up=%.3f | Neutral=%.3f",
                    f1_per_cls[0], f1_per_cls[1], f1_per_cls[2])
    except Exception:
        pass

    # 2.3 –û—Ç—á—ë—Ç—ã –∏ –≥—Ä–∞—Ñ–∏–∫–∏ –ø–æ eval
    labels_order = [0, 1, 2]
    target_names = ["Down", "Up", "Neutral"]
    report = classification_report(
        y_test_np, y_pred_eval, labels=labels_order, target_names=target_names, zero_division=0
    )
    (OUTPUT_DIR / f"{tag}_report.txt").write_text(report)

    ConfusionMatrixDisplay.from_predictions(y_test_np, y_pred_eval, cmap='viridis')
    plt.title(f"Confusion Matrix ({tag})")
    plt.savefig(OUTPUT_DIR / f"conf_matrix_{tag}.png")
    plt.close()

    # —É–≤–µ—Ä–µ–Ω–Ω—ã–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –Ω–∞ eval (+ coverage –≤ –ª–æ–≥)
    for th in CONFIDENCE_THRESHOLDS:
        mask = conf_eval >= th
        coverage = float(mask.mean())
        if mask.any():
            acc = accuracy_score(y_test_np[mask], y_pred_eval[mask])
            f1m = f1_score(y_test_np[mask], y_pred_eval[mask], average='macro')
            logger.info(f"[Conf >= {th:.2f}] Coverage: {coverage:.3f} | Acc: {acc:.4f} | F1 macro: {f1m:.4f}")
        else:
            logger.warning(f"[Conf >= {th:.2f}] Coverage: 0.000 ‚Äî –Ω–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤")

    # –≥–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ eval (–ø–æ –∫–ª–∞—Å—Å–∞–º, —É–∂–µ –ø–æ—Å–ª–µ T)
    try:
        plt.figure(figsize=(8, 5))
        for cls, name in zip([0, 1, 2], ["Down", "Up", "Neutral"]):
            if np.any(y_test_np == cls):
                plt.hist(conf_eval[y_test_np == cls], bins=30, alpha=0.5, label=name)
        plt.legend()
        plt.title(f"Confidence distribution (eval) ‚Äî {symbol}")
        out_hist = OUTPUT_DIR / f"conf_dist_{symbol}_{ts or 'run'}.png"
        plt.tight_layout()
        plt.savefig(out_hist)
        plt.close()
        logger.info("Saved confidence hist: %s", out_hist)
    except Exception as e:
        logger.warning("Failed to save confidence hist: %s", e)
    # =========================
    # 3) –ü–æ–∏—Å–∫ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞ –ø–æ precision
    # =========================
    try:
        th_auto, cov_auto = find_threshold_for_precision(y_test_np, proba_eval, target_precision=0.6)
        if th_auto is not None:
            logger.info("[Auto-threshold] Precision‚â•0.6 ‚Üí th=%.2f | coverage=%.3f", th_auto, cov_auto)
        else:
            logger.warning("[Auto-threshold] –ù–µ –Ω–∞–π–¥–µ–Ω –ø–æ—Ä–æ–≥ –¥–ª—è precision‚â•0.6")
    except Exception as e:
        logger.warning("Auto-threshold search failed: %s", e)

    # =========================
    # 3) –ö–∞–ª–∏–±—Ä–æ–≤–∫–∞ –Ω–∞ HOLDOUT (–µ—Å–ª–∏ –µ—Å—Ç—å) ‚Äî –Ω–∞ temperature-scaled –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç—è—Ö
    # =========================
    if calib is not None and proba_hold is not None and y_hold_np is not None:
        y_pred_hold = proba_hold.argmax(axis=1).astype(int)
        conf_hold   = proba_hold.max(axis=1)
        is_correct  = (y_pred_hold == y_hold_np).astype(int)

        ir = fit_confidence_calibrator(conf_hold, is_correct)

        save_calibrator(ir, path=str(run_dir / "confidence_calibrator.pkl"))
        save_calibrator(ir, path=str(MODEL_DIR / f"confidence_calibrator_{symbol}.pkl"))
        save_calibrator(ir, path=str(MODEL_DIR / "confidence_calibrator.pkl"))
        logger.info(
            "Saved confidence calibrator ‚Üí %s, and aliases: %s, %s",
            str(run_dir / "confidence_calibrator.pkl"),
            str(MODEL_DIR / f"confidence_calibrator_{symbol}.pkl"),
            str(MODEL_DIR / "confidence_calibrator.pkl"),
        )


def _timestamp() -> str:
    return datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")


def _archive_artifacts(symbol: str, ts: str):
    """
    –ö–æ–ø–∏—Ä—É–µ–º —Å–≤–µ–∂–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Ç—Ä–µ–Ω–µ—Ä–∞ –≤ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É:
    models/{symbol}/{ts}/(model.cbm, scaler.pkl, cat_features.pkl)
    + –ø–ª–æ—Å–∫–∏–µ —Ñ–∞–π–ª—ã model_{symbol}.cbm, scaler_{symbol}.pkl, cat_features_{symbol}.pkl
    """
    # –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ‚Äî –≥–¥–µ –∏—Ö —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç model_trainer
    model_src = MODEL_DIR / "saved_model.cbm"
    scaler_src = MODEL_DIR / "scaler.pkl"          # —Å–º. –æ–±–Ω–æ–≤–ª—ë–Ω–Ω—ã–π save –≤ prepare_data()
    cat_src = MODEL_DIR / "cat_features.pkl"

    target_dir = MODEL_DIR / symbol / ts
    target_dir.mkdir(parents=True, exist_ok=True)

    def _safe_copy(src: Path, dst: Path, label: str):
        try:
            if src.exists():
                shutil.copyfile(src, dst)
                logger.info(f"{label} —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤ {dst}")
            else:
                logger.warning(f"{label} –Ω–µ –Ω–∞–π–¥–µ–Ω: {src}")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å {label}: {e}")

    # –≤ timestamp-–ø–∞–ø–∫—É
    _safe_copy(model_src, target_dir / "model.cbm", "–ú–æ–¥–µ–ª—å")
    _safe_copy(scaler_src, target_dir / "scaler.pkl", "–°–∫–µ–π–ª–µ—Ä")
    _safe_copy(cat_src,    target_dir / "cat_features.pkl", "cat_features")

    # –ø–ª–æ—Å–∫–∏–µ –∞–ª–∏–∞—Å—ã –ø–æ —Å–∏–º–≤–æ–ª—É (—É–¥–æ–±–Ω–æ –∏—Å–∫–∞—Ç—å)
    _safe_copy(model_src, MODEL_DIR / f"model_{symbol}.cbm", "–ú–æ–¥–µ–ª—å (–∞–ª–∏–∞—Å)")
    _safe_copy(scaler_src, MODEL_DIR / f"scaler_{symbol}.pkl", "–°–∫–µ–π–ª–µ—Ä (–∞–ª–∏–∞—Å)")
    _safe_copy(cat_src,    MODEL_DIR / f"cat_features_{symbol}.pkl", "cat_features (–∞–ª–∏–∞—Å)")

    return target_dir


def save_metadata(symbol: str, ts: str, best_params: dict, extras: dict = None):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–º –ø—Ä–æ–≥–æ–Ω–µ –≤ models/metadata.json
    """
    meta_path = MODEL_DIR / "metadata.json"
    metadata = {}

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    if meta_path.exists():
        try:
            with open(meta_path, "r", encoding="utf-8") as f:
                metadata = json.load(f)
        except Exception:
            logger.warning("‚ö†Ô∏è metadata.json –ø–æ–≤—Ä–µ–∂–¥–µ–Ω ‚Äî —Å–æ–∑–¥–∞—é –∑–∞–Ω–æ–≤–æ.")
            metadata = {}

    run_entry = {
        "symbol": symbol,
        "timestamp": ts,
        "datetime": datetime.utcnow().isoformat(),
        "best_params": best_params,
        "purged_cv": {
            "n_splits": 3,
            "embargo": EMBARGO_BARS,
            "min_train": MIN_CV_TRAIN,
            "min_val": MIN_CV_VAL
        },
        "feature_set_version": "v1",
        "calibration_set": "holdout_30pct",
        "model_type": "symbol_specific",
    }

    if extras:
        run_entry.update(extras)

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å –ø–æ–¥ —Å–∏–º–≤–æ–ª
    metadata.setdefault(symbol, []).append(run_entry)

    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=4, ensure_ascii=False)

    logger.info("üßæ Metadata saved for %s ‚Üí %s", symbol, meta_path)


def train_on_symbol(symbol: str, interval: str = "15", threshold: float = 0.0015, use_rolling_cv: bool = True):
    logger.info(f"‚öôÔ∏è –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è {symbol}‚Ä¶")
    ts = _timestamp()

    # 1) –¥–∞–Ω–Ω—ã–µ
    X_train, X_test, y_train, y_test = prepare_data(symbol, interval, threshold)
    logger.info("Train size: %d | Test size: %d", len(X_train), len(X_test))
    # --- –ì–ê–†–î: –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—ã X/y (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π) ---
    if len(X_train) != len(y_train):
        n_safe = min(len(X_train), len(y_train))
        logger.warning("[Pipeline] Train length mismatch: X=%d, y=%d ‚Üí aligning to %d",
                       len(X_train), len(y_train), n_safe)
        X_train = X_train.iloc[:n_safe].reset_index(drop=True)
        y_train = y_train.iloc[:n_safe].reset_index(drop=True)

    if len(X_test) != len(y_test):
        n_safe = min(len(X_test), len(y_test))
        logger.warning("[Pipeline] Test length mismatch: X=%d, y=%d ‚Üí aligning to %d",
                       len(X_test), len(y_test), n_safe)
        X_test = X_test.iloc[:n_safe].reset_index(drop=True)
        y_test = y_test.iloc[:n_safe].reset_index(drop=True)

    # 2) –±—ã—Å—Ç—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–æ–≤
    best_params = optimize_catboost(X_train, y_train)
    logger.info("üîß Best hyperparams: %s", {k: (int(v) if k == "depth" else float(v)) for k, v in best_params.items()})

    # 3) rolling CV (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –∏ –µ—Å–ª–∏ —Ö–≤–∞—Ç–∞–µ—Ç –¥–∞–Ω–Ω—ã—Ö)
    MIN_TRAIN_FOR_ROLLING = 400  # –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥, —á—Ç–æ–±—ã –Ω–µ —Ç—Ä–∞—Ç–∏—Ç—å –≤—Ä–µ–º—è –Ω–∞ –∫—Ä–æ—à–µ—á–Ω—ã—Ö –≤—ã–±–æ—Ä–∫–∞—Ö
    if use_rolling_cv and len(X_train) >= MIN_TRAIN_FOR_ROLLING:
        scores = rolling_cross_validation(X_train, y_train, best_params, n_splits=5)
        logger.info("üìà Rolling CV F1_macro: %.4f", float(np.mean(scores)))
    elif use_rolling_cv:
        logger.warning("üöß Rolling CV –ø—Ä–æ–ø—É—â–µ–Ω (train=%d < %d)", len(X_train), MIN_TRAIN_FOR_ROLLING)

    # 4) —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (model_trainer —Å–∞–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –≤ models/)
    train_final_model(X_train, y_train, best_params)

    # 5) –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∞—Ä—Ö–∏–≤–∏—Ä—É–µ–º —Å–≤–µ–∂–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –ø–æ–¥ —Å–∏–º–≤–æ–ª/—Ç–∞–π–º—Å—Ç–µ–º–ø
    run_dir = _archive_artifacts(symbol, ts)

    # 6) –∑–∞–≥—Ä—É–∂–∞–µ–º —Å–≤–µ–∂—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (fail-safe)
    model = cb.CatBoostClassifier()
    model_path = MODEL_DIR / "saved_model.cbm"
    try:
        model.load_model(str(model_path))
    except Exception as e:
        logger.exception("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: %s", model_path)
        raise
    # 7) –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–≥–æ–Ω–µ
    save_metadata(symbol, ts, best_params)

    # === –ß–ï–°–¢–ù–ê–Ø –û–¶–ï–ù–ö–ê –ò –ö–ê–õ–ò–ë–†–û–í–ö–ê ===
    # –¥–µ–ª–∏–º —Ç–µ—Å—Ç –≤–æ –≤—Ä–µ–º–µ–Ω–∏: –ø–µ—Ä–≤—ã–µ 70% ‚Üí –º–µ—Ç—Ä–∏–∫–∏, –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30% ‚Üí –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞
    n_test = len(X_test)
    cut = max(1, int(n_test * 0.7))  # 70/30
    X_eval, y_eval = X_test.iloc[:cut], y_test.iloc[:cut]
    X_hold, y_hold = X_test.iloc[cut:], y_test.iloc[cut:]
    pct_eval = 100.0 * len(X_eval) / max(1, n_test)
    pct_hold = 100.0 * len(X_hold) / max(1, n_test)
    logger.info("[Eval/Holdout] sizes: eval=%d (%.1f%%), holdout=%d (%.1f%%), total=%d",
                len(X_eval), pct_eval, len(X_hold), pct_hold, n_test)

    # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞ —Ä–∞–∑–º–µ—Ä holdout –¥–ª—è –∏–∑–æ—Ç–æ–Ω–∏–∫–∏ (–∏–Ω–∞—á–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞–ª–∏–±—Ä–æ–≤–∫—É)
    MIN_HOLDOUT = 30
    if len(X_hold) >= MIN_HOLDOUT:
        calib_tuple = (X_hold, y_hold)
    else:
        logger.warning("Holdout —Å–ª–∏—à–∫–æ–º –º–∞–ª –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ (len=%d < %d) ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é –∫–∞–ª–∏–±—Ä–∞—Ç–æ—Ä",
                       len(X_hold), MIN_HOLDOUT)
        calib_tuple = None

    # –≤–∞–∂–Ω–æ: symbol ¬´–∫–∞–∫ –µ—Å—Ç—å¬ª, ts –ø–µ—Ä–µ–¥–∞—ë–º –æ—Ç–¥–µ–ª—å–Ω—ã–º –∞—Ä–≥—É–º–µ–Ω—Ç–æ–º (–±–µ–∑ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è)
    evaluate_model(model, X_eval, y_eval, symbol=symbol, ts=ts, calib=calib_tuple)


if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Bybit –∫–ª–∏–µ–Ω—Ç–∞ (–¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ OHLCV)
    client = HTTP(api_key=BYBIT_API_KEY, api_secret=BYBIT_API_SECRET)
    set_client(client)

    # –í—Ä–µ–º–µ–Ω–Ω–æ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ ‚Äî –ø–æ–∑–∂–µ –∑–∞–º–µ–Ω–∏–º –Ω–∞ top_pairs –∏–∑ pair_discovery
    symbols = ["BTCUSDT", "ETHUSDT"]

    for sym in symbols:
        train_on_symbol(sym)
