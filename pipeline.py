# pipeline.py
import os
import pandas as pd
import shutil
import pickle
import logging
from datetime import datetime, timezone
from pathlib import Path

import numpy as np
import matplotlib.pyplot as plt
import catboost as cb
from sklearn.metrics import (
    accuracy_score, classification_report, f1_score,
    ConfusionMatrixDisplay, precision_score
)

from pybit.unified_trading import HTTP
from config import BYBIT_API_KEY, BYBIT_API_SECRET, CONFIDENCE_THRESHOLDS
from data_loader import set_client

from model_trainer import (
    prepare_data,
    optimize_catboost,
    train_final_model,
    rolling_cross_validation, _atomic_write_text,
)
from confidence_calibrator import fit_confidence_calibrator, save_calibrator
import json
from datetime import datetime
from config import EMBARGO_BARS, MIN_CV_TRAIN, MIN_CV_VAL
from model_trainer import _sanitize_categoricals



# --- setup ---
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")
logger = logging.getLogger(__name__)

OUTPUT_DIR = Path("outputs")
MODEL_DIR = Path("models")
OUTPUT_DIR.mkdir(exist_ok=True)
MODEL_DIR.mkdir(exist_ok=True)


# === Align helper: ensures test/holdout columns match training features ===
import pickle
import catboost as cb


def _apply_temperature_scaling(proba: np.ndarray, T: float) -> np.ndarray:
    """Softmax temperature scaling for multi-class, from probabilities (logit-free)."""
    logits = np.log(np.clip(proba, 1e-12, 1.0))
    logits_T = logits / max(T, 1e-6)
    m = np.max(logits_T, axis=1, keepdims=True)
    exp = np.exp(logits_T - m)
    return exp / np.sum(exp, axis=1, keepdims=True)

def _nll_multiclass(y_true: np.ndarray, proba: np.ndarray) -> float:
    """
    NLL –¥–ª—è –º—É–ª—å—Ç–∏-–∫–ª–∞—Å—Å–∞, —É—Å—Ç–æ–π—á–∏–≤—ã–π –∫ —Å–∏—Ç—É–∞—Ü–∏–∏,
    –∫–æ–≥–¥–∞ –º–æ–¥–µ–ª—å –≤–µ—Ä–Ω—É–ª–∞ –º–µ–Ω—å—à–µ –∫–ª–∞—Å—Å–æ–≤, —á–µ–º –µ—Å—Ç—å –≤ –≥–ª–æ–±–∞–ª—å–Ω–æ–π —Ä–∞–∑–º–µ—Ç–∫–µ.
    –ò–≥–Ω–æ—Ä–∏—Ä—É–µ–º –æ–±—ä–µ–∫—Ç—ã, –¥–ª—è –∫–æ—Ç–æ—Ä—ã—Ö y_true >= proba.shape[1].
    """
    y_true = np.asarray(y_true, dtype=int)
    proba = np.asarray(proba, dtype=float)

    n_classes = proba.shape[1]
    valid_mask = (y_true >= 0) & (y_true < n_classes)

    if not np.any(valid_mask):
        # –Ω–µ—Ç –Ω–∏ –æ–¥–Ω–æ–≥–æ –æ–±—ä–µ–∫—Ç–∞ —Å –º–µ—Ç–∫–æ–π, –ø–æ–ø–∞–¥–∞—é—â–µ–π –≤ –¥–∏–∞–ø–∞–∑–æ–Ω –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤
        # –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –±–æ–ª—å—à–æ–π NLL, —á—Ç–æ–±—ã —Ç–∞–∫–æ–π T —Ç–æ—á–Ω–æ –Ω–µ —Å—Ç–∞–ª ¬´–ª—É—á—à–∏–º¬ª
        return 1e9

    yv = y_true[valid_mask]
    pv = proba[valid_mask]

    p = np.clip(pv[np.arange(len(yv)), yv], 1e-12, 1.0)
    return float(-np.mean(np.log(p)))


def _find_best_temperature(proba_val: np.ndarray, y_val: np.ndarray,
                           t_min: float, t_max: float, t_step: float) -> float:
    best_T, best_nll = 1.0, _nll_multiclass(y_val, proba_val)
    T = t_min
    while T <= t_max + 1e-9:
        proba_T = _apply_temperature_scaling(proba_val, T)
        nll = _nll_multiclass(y_val, proba_T)
        if nll < best_nll:
            best_T, best_nll = float(T), float(nll)
        T += t_step
    return best_T


def find_threshold_for_precision(y_true, proba, target_precision=0.6):
    """
    –ü–æ–¥–±–∏—Ä–∞–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º precision >= target_precision.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç (threshold, coverage).
    """
    from sklearn.metrics import precision_score

    conf = proba.max(axis=1)
    y_pred = proba.argmax(axis=1)
    for th in np.linspace(0.9, 0.3, 13):  # —à–∞–≥ 0.05 –≤–Ω–∏–∑ (0.9 ‚Üí 0.3)
        m = conf >= th
        if not m.any():
            continue
        prec = precision_score(y_true[m], y_pred[m], average="macro", zero_division=0)
        if prec >= target_precision:
            return th, m.mean()  # –Ω–∞–π–¥–µ–Ω–Ω—ã–π –ø–æ—Ä–æ–≥ –∏ –¥–æ–ª—è –æ—Ö–≤–∞—Ç–∞
    return None, 0.0


# ===== Inference alignment helpers =====
def _load_feature_and_cat_lists(symbol: str | None = None):
    """
    –ì—Ä—É–∑–∏–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –∏ –∫–∞—Ç-—Ñ–∏—á–µ–π, —Å–æ—Ö—Ä–∞–Ω—ë–Ω–Ω—ã—Ö –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏.
    –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç: models/<SYMBOL>/<ts>/feature_columns.pkl (–µ—Å–ª–∏ —Ö–æ—á–µ—à—å ‚Äî –¥–æ–±–∞–≤—å),
               models/feature_columns.pkl,
               models/feature_columns_<SYMBOL>.pkl (–µ—Å–ª–∏ –≤–µ–¥—ë—à—å –ø–æ-—Å–∏–º–≤–æ–ª—å–Ω–æ).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: (feature_columns: list[str] | None, cat_features: list[str] | None)
    """
    candidates = []
    if symbol:
        candidates += [
            MODEL_DIR / f"feature_columns_{symbol}.pkl",
        ]
    candidates += [
        MODEL_DIR / "feature_columns.pkl",
    ]
    feat_cols = None
    for p in candidates:
        if p.exists():
            with open(p, "rb") as f:
                feat_cols = pickle.load(f)
            break

    # cat_features
    cat_list = None
    cat_paths = []
    if symbol:
        cat_paths += [MODEL_DIR / f"cat_features_{symbol}.pkl"]
    cat_paths += [MODEL_DIR / "cat_features.pkl"]
    for p in cat_paths:
        if p.exists():
            with open(p, "rb") as f:
                cat_list = pickle.load(f)
            break

    return feat_cols, cat_list

def _align_for_infer(X: pd.DataFrame, symbol: str | None = None):
    """
    –î–µ–ª–∞–µ—Ç –≤—Ö–æ–¥–Ω–æ–π X —Å–æ–≤–º–µ—Å—Ç–∏–º—ã–º —Å –º–æ–¥–µ–ª—å—é:
    - –≤—ã–±–∏—Ä–∞–µ—Ç –¢–ï –ñ–ï –∫–æ–ª–æ–Ω–∫–∏ –≤ –¢–û–ú –ñ–ï –ø–æ—Ä—è–¥–∫–µ, —á—Ç–æ –∏ –Ω–∞ –æ–±—É—á–µ–Ω–∏–∏;
    - –¥–æ–±–∞–≤–ª—è–µ—Ç –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ (0.0 –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö, '__NA__' –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö);
    - –ø—Ä–∏–≤–æ–¥–∏—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫ pandas StringDtype;
    - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (X_aligned, cat_idx).
    """
    X_in = X.copy()

    # 1) –≥—Ä—É–∑–∏–º —ç—Ç–∞–ª–æ–Ω–Ω—ã–µ —Å–ø–∏—Å–∫–∏
    feat_cols, cat_features = _load_feature_and_cat_lists(symbol)

    # –µ—Å–ª–∏ —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫ –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ, —á—Ç–æ –ø—Ä–∏—à–ª–æ (–Ω–æ —ç—Ç–æ —Ä–∏—Å–∫!)
    if feat_cols is None:
        feat_cols = X_in.columns.tolist()
        logger.warning(
            "[AlignInfer] feature_columns.pkl –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Üí –∏—Å–ø–æ–ª—å–∑—É–µ–º –≤—Ö–æ–¥–Ω—ã–µ –∫–æ–ª–æ–Ω–∫–∏ –∫–∞–∫ –µ—Å—Ç—å (‚ö† —Ä–∏—Å–∫ —Ä–∞—Å—Å–∏–Ω—Ö—Ä–æ–Ω–∞)"
        )

    # 2) –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –Ω–∞–ª–∏—á–∏–µ –≤—Å–µ—Ö –Ω—É–∂–Ω—ã—Ö –∫–æ–ª–æ–Ω–æ–∫
    exist_cols = set(X_in.columns)
    need_cols = list(feat_cols)

    # –µ—Å–ª–∏ —Å–ø–∏—Å–æ–∫ cat_features –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Äî –æ–ø—Ä–µ–¥–µ–ª–∏–º –ø–æ dtype –≤—Ö–æ–¥–Ω–æ–≥–æ X (best-effort)
    if cat_features is None:
        cat_features = X_in.select_dtypes(include=["object", "category", "string"]).columns.tolist()
        logger.warning(
            "[AlignInfer] cat_features.pkl –Ω–µ –Ω–∞–π–¥–µ–Ω ‚Üí fallback –ø–æ dtype=%s (‚ö† int-–∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –Ω–µ –±—É–¥—É—Ç –æ–ø—Ä–µ–¥–µ–ª–µ–Ω—ã)",
            cat_features[:10]
        )

    cat_set = set(cat_features)

    for col in need_cols:
        if col not in exist_cols:
            if col in cat_set:
                X_in[col] = "__NA__"
            else:
                X_in[col] = 0.0

    # 3) —Ä–µ–∂–µ–º –ª–∏—à–Ω–∏–µ –∫–æ–ª–æ–Ω–∫–∏ –∏ —Å—Ç–∞–≤–∏–º –ü–†–ê–í–ò–õ–¨–ù–´–ô –ü–û–†–Ø–î–û–ö
    X_in = X_in[need_cols]

    # 4) —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö (string + fillna("__NA__"))
    cat_cols_present = [c for c in cat_features if c in X_in.columns]
    if cat_cols_present:
        X_in.loc[:, cat_cols_present] = X_in.loc[:, cat_cols_present].astype("string").fillna("__NA__")

    # 5) –∏–Ω–¥–µ–∫—Å—ã –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö (CatBoost –æ–∂–∏–¥–∞–µ—Ç –∏–Ω–¥–µ–∫—Å—ã/–ø–æ–∑–∏—Ü–∏–∏)
    cat_idx = [X_in.columns.get_loc(c) for c in cat_cols_present]

    logger.info(
        "[AlignInfer] aligned %d features (added=%d, cats=%d)",
        len(need_cols),
        len(set(need_cols) - exist_cols),
        len(cat_idx)
    )

    return X_in, cat_idx


logger = logging.getLogger(__name__)

def find_threshold_for_precision(
    y_true,
    proba,
    target_precision: float = 0.6,
    calibrator=None,
    T: float = 1.0,
    thresholds: np.ndarray | None = None,
):
    """
    –ù–∞—Ö–æ–¥–∏—Ç –ú–ò–ù–ò–ú–ê–õ–¨–ù–´–ô –ø–æ—Ä–æ–≥ –ø–æ –ö–ê–õ–ò–ë–†–û–í–ê–ù–ù–û–ô —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏, –ø—Ä–∏ –∫–æ—Ç–æ—Ä–æ–º macro-precision >= target_precision.
    –ü–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –ø—Ä–æ–¥-–ª–æ–≥–∏–∫–µ:
        proba -> Temperature (T) -> max(prob) -> Isotonic (–µ—Å–ª–∏ –µ—Å—Ç—å)

    –ü–∞—Ä–∞–º–µ—Ç—Ä—ã:
      y_true        : true labels (array-like)
      proba         : –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ (n_samples x n_classes)
      target_precision : —Ü–µ–ª–µ–≤–æ–π macro-precision
      calibrator    : sklearn.isotonic.IsotonicRegression (–∏–ª–∏ None)
      T             : —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ (float)
      thresholds    : np.ndarray –ø–æ—Ä–æ–≥–æ–≤; –µ—Å–ª–∏ None -> linspace(0.90..0.30, —à–∞–≥ ‚âà 0.05)

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      (threshold, coverage)  ‚Äî –µ—Å–ª–∏ –Ω–∞–π–¥–µ–Ω –ø–æ—Ä–æ–≥; –∏–Ω–∞—á–µ (None, 0.0)
    """
    # --- –≤–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–∞
    if proba is None or y_true is None:
        logger.warning("[find_threshold_for_precision] y_true/proba is None ‚Äî returning (None, 0.0)")
        return None, 0.0

    y_true = np.asarray(y_true)
    proba = np.asarray(proba)

    if y_true.size == 0 or proba.size == 0:
        logger.warning("[find_threshold_for_precision] Empty input ‚Äî returning (None, 0.0)")
        return None, 0.0

    if y_true.shape[0] != proba.shape[0]:
        n = min(y_true.shape[0], proba.shape[0])
        logger.warning("[find_threshold_for_precision] Length mismatch (y=%d, p=%d) ‚Üí truncated to %d",
                       y_true.shape[0], proba.shape[0], n)
        y_true = y_true[:n]
        proba = proba[:n]

    # --- Temperature scaling
    proba_T = _apply_temperature_scaling(proba, T)  # –∏—Å–ø–æ–ª—å–∑—É–π –≤–∞—à—É —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é –∏–∑ pipeline
    conf = proba_T.max(axis=1)
    y_pred = proba_T.argmax(axis=1)

    # --- Isotonic calibration (–ø–æ —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏)
    if calibrator is not None:
        try:
            # –≤–∞—à–∞ helper-—Ñ—É–Ω–∫—Ü–∏—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏: –¥–æ–ª–∂–Ω–∞ –≤–µ—Ä–Ω—É—Ç—å np.ndarray –≤ [0, 1]
            from model_trainer import apply_isotonic_confidence  # –µ—Å–ª–∏ —É–∂–µ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤—ã—à–µ ‚Äî –º–æ–∂–Ω–æ —É–±—Ä–∞—Ç—å
            conf = apply_isotonic_confidence(calibrator, conf)
        except Exception as e:
            logger.warning("[find_threshold_for_precision] Isotonic failed (%s) ‚Äî using raw confidence", e)

    # --- —Å–µ—Ç–∫–∞ –ø–æ—Ä–æ–≥–æ–≤
    if thresholds is None:
        thresholds = np.linspace(0.90, 0.30, 13)

    best_prec = -1.0
    best_th = None
    best_cov = 0.0

    for th in thresholds:
        m = conf >= th
        if not m.any():
            continue
        prec = precision_score(y_true[m], y_pred[m], average="macro", zero_division=0)
        cov = float(m.mean())

        if prec >= target_precision:
            logger.info("[Auto-threshold] target_precision=%.2f ‚Üí th=%.2f | precision=%.3f | coverage=%.3f",
                        target_precision, th, prec, cov)
            return float(th), cov

        if prec > best_prec:
            best_prec, best_th, best_cov = float(prec), float(th), cov

    logger.warning("[Auto-threshold] No threshold reached target_precision=%.2f (best=%.3f @ th=%.2f, cov=%.3f)",
                   target_precision, max(0.0, best_prec), best_th if best_th is not None else -1.0, best_cov)
    return None, 0.0

def _archive_artifacts(symbol: str, ts: str):
    """
    –ö–æ–ø–∏—Ä—É–µ–º —Å–≤–µ–∂–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã —Ç—Ä–µ–Ω–µ—Ä–∞ –≤ –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω—É—é –ø–∞–ø–∫—É:
    models/{symbol}/{ts}/(model.cbm, scaler.pkl, cat_features.pkl, feature_columns.pkl)
    + –ø–ª–æ—Å–∫–∏–µ —Ñ–∞–π–ª—ã model_{symbol}.cbm, scaler_{symbol}.pkl, cat_features_{symbol}.pkl, feature_columns_{symbol}.pkl
    """
    # –∏—Å—Ç–æ—á–Ω–∏–∫–∏ ‚Äî –≥–¥–µ –∏—Ö —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç model_trainer / prepare_data
    model_src = MODEL_DIR / "saved_model.cbm"
    scaler_src = MODEL_DIR / "scaler.pkl"              # —Å–º. —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ prepare_data()
    cat_src    = MODEL_DIR / "cat_features.pkl"
    feat_src   = MODEL_DIR / "feature_columns.pkl"     # –∫—Ä–∏—Ç–∏—á–Ω–æ –¥–ª—è –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è —Ñ–∏—á–µ–π –Ω–∞ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ

    target_dir = MODEL_DIR / symbol / ts
    target_dir.mkdir(parents=True, exist_ok=True)

    def _safe_copy(src: Path, dst: Path, label: str):
        try:
            if src.exists():
                shutil.copyfile(src, dst)
                logger.info(f"{label} —Å–∫–æ–ø–∏—Ä–æ–≤–∞–Ω –≤ {dst}")
            else:
                logger.warning(f"{label} –Ω–µ –Ω–∞–π–¥–µ–Ω: {src}")
        except Exception as e:
            logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å {label}: {e}")

    # --- –≤ timestamp-–ø–∞–ø–∫—É (run_dir) ---
    _safe_copy(model_src, target_dir / "model.cbm",            "–ú–æ–¥–µ–ª—å")
    _safe_copy(scaler_src, target_dir / "scaler.pkl",          "–°–∫–µ–π–ª–µ—Ä")
    _safe_copy(cat_src,    target_dir / "cat_features.pkl",    "cat_features")
    _safe_copy(feat_src,   target_dir / "feature_columns.pkl", "feature_columns")

    # --- –ø–ª–æ—Å–∫–∏–µ –∞–ª–∏–∞—Å—ã –ø–æ —Å–∏–º–≤–æ–ª—É ---
    _safe_copy(model_src, MODEL_DIR / f"model_{symbol}.cbm",             "–ú–æ–¥–µ–ª—å (–∞–ª–∏–∞—Å)")
    _safe_copy(scaler_src, MODEL_DIR / f"scaler_{symbol}.pkl",           "–°–∫–µ–π–ª–µ—Ä (–∞–ª–∏–∞—Å)")
    _safe_copy(cat_src,    MODEL_DIR / f"cat_features_{symbol}.pkl",     "cat_features (–∞–ª–∏–∞—Å)")
    _safe_copy(feat_src,   MODEL_DIR / f"feature_columns_{symbol}.pkl",  "feature_columns (–∞–ª–∏–∞—Å)")

    return target_dir


def evaluate_model(model, X_test, y_test, symbol="model", ts=None, calib=None):
    """
    –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª—å –Ω–∞ TEST (eval) –∏, –µ—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω calib=(X_hold, y_hold),
    –ø–æ–¥–±–∏—Ä–∞–µ—Ç Temperature T –ø–æ NLL –Ω–∞ holdout (—Å safeguard –ø–æ F1), –ø—Ä–∏–º–µ–Ω—è–µ—Ç –µ–≥–æ,
    –æ–±—É—á–∞–µ—Ç IsotonicRegression –Ω–∞ conf(holdout) –∏ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∏ T, –∏ –∫–∞–ª–∏–±—Ä–∞—Ç–æ—Ä.

    –í—Å–µ –º–µ—Ç—Ä–∏–∫–∏/–ø–æ—Ä–æ–≥–∏ —Å—á–∏—Ç–∞—é—Ç—Å—è –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ —Å –ø—Ä–æ–¥-–ª–æ–≥–∏–∫–æ–π:
      proba -> Temperature -> max(proba) -> Isotonic.predict(conf).
    """
    from config import (
        CONFIDENCE_THRESHOLDS,
        TEMPERATURE_SCALING, TEMPERATURE_MIN, TEMPERATURE_MAX, TEMPERATURE_STEP,
    )
    from confidence_calibrator import fit_confidence_calibrator, save_calibrator

    # --- –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º –∫–∞—Ç–∞–ª–æ–≥–∏
    try:
        OUTPUT_DIR.mkdir(parents=True, exist_ok=True)
    except Exception:
        pass

    symbol = symbol or "model"
    tag = f"{symbol}" + (f"_{ts}" if ts else "")
    run_dir = MODEL_DIR / symbol / (ts if ts else "")
    run_dir.mkdir(parents=True, exist_ok=True)

    # ---- helpers
    def _log_conf_stats(name: str, proba: np.ndarray):
        conf = proba.max(axis=1)
        logger.info("[%s] mean_conf=%.3f | p90=%.3f | p95=%.3f | max=%.3f",
                    name,
                    float(conf.mean()),
                    float(np.quantile(conf, 0.90)),
                    float(np.quantile(conf, 0.95)),
                    float(conf.max()))

    def _apply_isotonic(conf: np.ndarray, calibrator):
        if calibrator is None:
            return conf
        try:
            out = calibrator.predict(conf.reshape(-1))
            out = np.asarray(out, dtype=float)
            return np.clip(out, 0.0, 1.0)
        except Exception as e:
            logger.warning("Isotonic application failed: %s ‚Äî using raw confidence", e)
            return conf

    def _atomic_write(path_obj, text_payload: str):
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–≤–æ–π _atomic_write_text, –µ—Å–ª–∏ –æ–Ω –µ—Å—Ç—å; –∏–Ω–∞—á–µ ‚Äî –æ–±—ã—á–Ω–∞—è –∑–∞–ø–∏—Å—å
        try:
            _atomic_write_text(path_obj, text_payload)
        except Exception:
            try:
                path_obj.write_text(text_payload)
            except Exception as e:
                logger.warning("Failed to write %s: %s", str(path_obj), e)

    # =========================
    # 1) HOLDOUT: –ø–æ–¥–±–æ—Ä —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—ã –∏ –æ–±—É—á–µ–Ω–∏–µ –∫–∞–ª–∏–±—Ä–∞—Ç–æ—Ä–∞
    # =========================
    best_T = 1.0
    proba_hold_T = None
    y_hold_np = None
    calibrator = None

    if calib is not None:
        X_hold, y_hold = calib

        # 1.1 –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º HOLDOUT –ø–æ–¥ –æ–±—É—á–∞—é—â–∏–π –ø–∞–π–ø–ª–∞–π–Ω (—Å—Ç—Ä–æ–≥–∏–π –ø–æ—Ä—è–¥–æ–∫/–Ω–∞–±–æ—Ä —Ñ–∏—á, cat idx)
        X_hold_aligned, cat_idx_hold = _align_for_infer(X_hold, symbol=symbol)
        pool_hold = cb.Pool(X_hold_aligned, cat_features=cat_idx_hold if cat_idx_hold else None)

        # 1.2 –°—ã—Ä—ã–µ –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ –Ω–∞ holdout
        proba_hold_raw = np.asarray(model.predict_proba(pool_hold))
        y_hold_np = np.asarray(y_hold).astype(int)

        _log_conf_stats("HOLD raw", proba_hold_raw)

        # 1.3 –ü–æ–¥–±–æ—Ä T –ø–æ NLL (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –∏ –¥–∞–Ω–Ω—ã—Ö –¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ)
        if TEMPERATURE_SCALING and len(y_hold_np) >= 30:
            T_min = float(TEMPERATURE_MIN)
            T_max = float(TEMPERATURE_MAX)
            T_step = float(TEMPERATURE_STEP)
            best_T = _find_best_temperature(proba_hold_raw, y_hold_np, T_min, T_max, T_step)
            logger.info("Temperature scaling: candidate T=%.2f (chosen on holdout by NLL)", best_T)

        # 1.4 Safeguard –ø–æ macro-F1
        y_pred_raw = proba_hold_raw.argmax(axis=1).astype(int)
        f1_raw = f1_score(y_hold_np, y_pred_raw, average="macro")

        proba_hold_T_cand = _apply_temperature_scaling(proba_hold_raw, best_T)
        y_pred_T = proba_hold_T_cand.argmax(axis=1).astype(int)
        f1_T = f1_score(y_hold_np, y_pred_T, average="macro")

        if f1_T + 1e-9 < f1_raw - 0.01:
            logger.info(
                "Temperature rollback: F1 holdout decreased (raw=%.4f -> T=%.4f). Using T=1.0",
                f1_raw, f1_T
            )
            best_T = 1.0
            proba_hold_T = proba_hold_raw
        else:
            proba_hold_T = proba_hold_T_cand

        _log_conf_stats("HOLD T", proba_hold_T)

        # 1.5 –°–æ—Ö—Ä–∞–Ω–∏—Ç—å T (–∏ –∞–ª–∏–∞—Å—ã) –∞—Ç–æ–º–∞—Ä–Ω–æ
        payload = json.dumps({"T": float(best_T)})
        _atomic_write(run_dir / "temperature.json", payload)
        _atomic_write(MODEL_DIR / f"temperature_{symbol}.json", payload)
        _atomic_write(MODEL_DIR / "temperature.json", payload)

        # 1.6 –û–±—É—á–∏—Ç—å –∫–∞–ª–∏–±—Ä–∞—Ç–æ—Ä —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ –Ω–∞ holdout (–ø–æ—Å–ª–µ Temperature)
        conf_hold = proba_hold_T.max(axis=1)
        is_correct = (proba_hold_T.argmax(axis=1).astype(int) == y_hold_np).astype(int)
        ir = fit_confidence_calibrator(conf_hold, is_correct)
        save_calibrator(ir, path=str(run_dir / "confidence_calibrator.pkl"))
        save_calibrator(ir, path=str(MODEL_DIR / f"confidence_calibrator_{symbol}.pkl"))
        save_calibrator(ir, path=str(MODEL_DIR / "confidence_calibrator.pkl"))
        calibrator = ir
    else:
        logger.info("[Eval] No holdout calibration provided ‚Äî using raw T=1.0 and no isotonic.")

    # =========================
    # 2) TEST/EVAL: –º–µ—Ç—Ä–∏–∫–∏ –∏ –æ—Ç—á—ë—Ç—ã (T –∏ –∏–∑–æ—Ç–æ–Ω–∏–∫ –∫–∞–∫ –≤ –ø—Ä–æ–¥–µ)
    # =========================
    # 2.1 –í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º TEST –ø–æ–¥ –æ–±—É—á–∞—é—â–∏–π –ø–∞–π–ø–ª–∞–π–Ω
    X_test_aligned, cat_idx_test = _align_for_infer(X_test, symbol=symbol)
    pool_test = cb.Pool(X_test_aligned, cat_features=cat_idx_test if cat_idx_test else None)

    # 2.2 –í–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ + Temperature
    proba_eval_raw = np.asarray(model.predict_proba(pool_test))
    proba_eval_T = _apply_temperature_scaling(proba_eval_raw, best_T)
    _log_conf_stats("EVAL T", proba_eval_T)

    # 2.3 –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ (–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω–∞—è) —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å
    y_test_np = np.asarray(y_test).astype(int)
    y_pred_eval = proba_eval_T.argmax(axis=1).astype(int)
    conf_eval_raw = proba_eval_T.max(axis=1)
    conf_eval_cal = _apply_isotonic(conf_eval_raw, calibrator)

    logger.info("=" * 30 + f" [FINAL METRICS] {tag} " + "=" * 30)
    acc_eval = accuracy_score(y_test_np, y_pred_eval)
    f1m_eval = f1_score(y_test_np, y_pred_eval, average="macro")
    logger.info("[All] Accuracy: %.4f", acc_eval)
    logger.info("[All] F1 macro: %.4f", f1m_eval)

    # ------- –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏–µ –∫–ª–∞—Å—Å—ã (2 –∏–ª–∏ 3) -------
    uniq = np.unique(y_test_np)
    # —Å–æ—Ä—Ç–∏—Ä—É–µ–º, —á—Ç–æ–±—ã 0,1,(2) —à–ª–∏ –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ
    labels_order = sorted(int(c) for c in uniq)
    name_map = {0: "Down", 1: "Up", 2: "Neutral"}
    target_names = [name_map.get(c, str(c)) for c in labels_order]

    # per-class F1 (–¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞)
    try:
        if len(labels_order) >= 2:
            from sklearn.metrics import precision_recall_fscore_support
            _, _, f1_per_cls, _ = precision_recall_fscore_support(
                y_test_np,
                y_pred_eval,
                labels=labels_order,
                zero_division=0,
            )
            log_parts = []
            for c, f1_c in zip(labels_order, f1_per_cls):
                log_parts.append(f"{name_map.get(c, c)}={f1_c:.3f}")
            logger.info("[Eval per-class] F1: " + " | ".join(log_parts))
        else:
            logger.warning("[Eval] Less than 2 unique classes in y_test ‚Äî skipping per-class metrics")
    except Exception as e:
        logger.warning("Per-class metrics failed: %s", e)

    # 2.4 –û—Ç—á—ë—Ç –∏ –º–∞—Ç—Ä–∏—Ü–∞ –æ—à–∏–±–æ–∫
    try:
        report = classification_report(
            y_test_np,
            y_pred_eval,
            labels=labels_order,
            target_names=target_names,
            zero_division=0,
        )
        (OUTPUT_DIR / f"{tag}_report.txt").write_text(report)
    except Exception as e:
        logger.warning("Failed to write classification report: %s", e)

    try:
        ConfusionMatrixDisplay.from_predictions(
            y_test_np, y_pred_eval, labels=labels_order, cmap="viridis"
        )
        plt.title(f"Confusion Matrix ({tag})")
        plt.savefig(OUTPUT_DIR / f"conf_matrix_{tag}.png")
        plt.close()
    except Exception as e:
        logger.warning("Failed to save confusion matrix: %s", e)

    # 2.5 –ú–µ—Ç—Ä–∏–∫–∏ –ø–æ –ø–æ—Ä–æ–≥–∞–º ‚Äî –Ω–∞ –ö–ê–õ–ò–ë–†–û–í–ê–ù–ù–û–ô —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    for th in CONFIDENCE_THRESHOLDS:
        mask = conf_eval_cal >= th
        coverage = float(mask.mean())
        if mask.any():
            acc_c = accuracy_score(y_test_np[mask], y_pred_eval[mask])
            f1m_c = f1_score(y_test_np[mask], y_pred_eval[mask], average="macro")
            logger.info(
                f"[Conf(cal) >= {th:.2f}] Coverage: {coverage:.3f} | "
                f"Acc: {acc_c:.4f} | F1 macro: {f1m_c:.4f}"
            )
        else:
            logger.warning(f"[Conf(cal) >= {th:.2f}] Coverage: 0.000 ‚Äî –Ω–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤")

    # 2.6 –ì–∏—Å—Ç–æ–≥—Ä–∞–º–º—ã —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏ (–∫–∞–ª–∏–±—Ä–æ–≤–∞–Ω–Ω–æ–π)
    try:
        plt.figure(figsize=(8, 5))
        for cls in labels_order:
            name = name_map.get(cls, str(cls))
            if np.any(y_test_np == cls):
                plt.hist(
                    conf_eval_cal[y_test_np == cls],
                    bins=30,
                    alpha=0.5,
                    label=name,
                )
        plt.legend()
        plt.title(f"Confidence distribution (eval, calibrated) ‚Äî {symbol}")
        out_hist = OUTPUT_DIR / f"conf_dist_{symbol}_{ts or 'run'}.png"
        plt.tight_layout()
        plt.savefig(out_hist)
        plt.close()
        logger.info("Saved confidence hist: %s", out_hist)
    except Exception as e:
        logger.warning("Failed to save confidence hist: %s", e)

    # =========================
    # 3) –ü–æ–∏—Å–∫ –∞–≤—Ç–æ-–ø–æ—Ä–æ–≥–∞ –ø–æ precision ‚Äî –Ω–∞ –ö–ê–õ–ò–ë–†–û–í–ê–ù–ù–û–ô —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏
    # =========================
    try:
        # –ø—Ä–æ–±—É–µ–º ¬´–Ω–æ–≤—É—é¬ª —Å–∏–≥–Ω–∞—Ç—É—Ä—É (—Å calibrator/T); –µ—Å–ª–∏ –Ω–µ—Ç ‚Äî fallback –Ω–∞ —Å—Ç–∞—Ä—É—é
        try:
            th_auto, cov_auto = find_threshold_for_precision(
                y_true=y_test_np,
                proba=proba_eval_T,           # —É–∂–µ –ø–æ—Å–ª–µ Temperature
                target_precision=0.60,
                calibrator=calibrator,        # —á—Ç–æ–±—ã —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–ª—Å—è calibrated confidence
                T=best_T,
            )
        except TypeError:
            # —Å—Ç–∞—Ä–∞—è –≤–µ—Ä—Å–∏—è –±–µ–∑ calibrator/T
            th_auto, cov_auto = find_threshold_for_precision(
                y_test_np, proba_eval_T, target_precision=0.60
            )

        if th_auto is not None:
            logger.info(
                "[Auto-threshold] Precision‚â•0.6 ‚Üí th=%.2f | coverage=%.3f",
                th_auto, cov_auto,
            )
        else:
            logger.warning("[Auto-threshold] –ù–µ –Ω–∞–π–¥–µ–Ω –ø–æ—Ä–æ–≥ –¥–ª—è precision‚â•0.6")
    except Exception as e:
        logger.warning("Auto-threshold search failed: %s", e)



def _timestamp() -> str:
    return datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")


def save_metadata(symbol: str, ts: str, best_params: dict, extras: dict = None):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–º –ø—Ä–æ–≥–æ–Ω–µ –≤ models/metadata.json
    """
    meta_path = MODEL_DIR / "metadata.json"
    metadata = {}

    # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–µ–¥—ã–¥—É—â–∏–µ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ, –µ—Å–ª–∏ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç
    if meta_path.exists():
        try:
            with open(meta_path, "r", encoding="utf-8") as f:
                metadata = json.load(f)
        except Exception:
            logger.warning("‚ö†Ô∏è metadata.json –ø–æ–≤—Ä–µ–∂–¥–µ–Ω ‚Äî —Å–æ–∑–¥–∞—é –∑–∞–Ω–æ–≤–æ.")
            metadata = {}

    run_entry = {
        "symbol": symbol,
        "timestamp": ts,
        "datetime": datetime.utcnow().isoformat(),
        "best_params": best_params,
        "purged_cv": {
            "n_splits": 3,
            "embargo": EMBARGO_BARS,
            "min_train": MIN_CV_TRAIN,
            "min_val": MIN_CV_VAL
        },
        "feature_set_version": "v1",
        "calibration_set": "holdout_30pct",
        "model_type": "symbol_specific",
    }

    if extras:
        run_entry.update(extras)

    # –î–æ–±–∞–≤–ª—è–µ–º –∑–∞–ø–∏—Å—å –ø–æ–¥ —Å–∏–º–≤–æ–ª
    metadata.setdefault(symbol, []).append(run_entry)

    with open(meta_path, "w", encoding="utf-8") as f:
        json.dump(metadata, f, indent=4, ensure_ascii=False)

    logger.info("üßæ Metadata saved for %s ‚Üí %s", symbol, meta_path)


def train_on_symbol(symbol: str, interval: str = "15", threshold: float = 0.0015, use_rolling_cv: bool = True):
    logger.info(f"‚öôÔ∏è –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –¥–ª—è {symbol}‚Ä¶")
    ts = _timestamp()

    # 1) –¥–∞–Ω–Ω—ã–µ
    X_train, X_test, y_train, y_test = prepare_data(symbol, interval, threshold)
    logger.info("Train size: %d | Test size: %d", len(X_train), len(X_test))
    # --- –ì–ê–†–î: –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—ã X/y (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π) ---
    if len(X_train) != len(y_train):
        n_safe = min(len(X_train), len(y_train))
        logger.warning("[Pipeline] Train length mismatch: X=%d, y=%d ‚Üí aligning to %d",
                       len(X_train), len(y_train), n_safe)
        X_train = X_train.iloc[:n_safe].reset_index(drop=True)
        y_train = y_train.iloc[:n_safe].reset_index(drop=True)

    if len(X_test) != len(y_test):
        n_safe = min(len(X_test), len(y_test))
        logger.warning("[Pipeline] Test length mismatch: X=%d, y=%d ‚Üí aligning to %d",
                       len(X_test), len(y_test), n_safe)
        X_test = X_test.iloc[:n_safe].reset_index(drop=True)
        y_test = y_test.iloc[:n_safe].reset_index(drop=True)

    # 2) –±—ã—Å—Ç—Ä–∞—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è –≥–∏–ø–µ—Ä–æ–≤
    best_params = optimize_catboost(X_train, y_train)
    logger.info("üîß Best hyperparams: %s", {k: (int(v) if k == "depth" else float(v)) for k, v in best_params.items()})

    # 3) rolling CV (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –∏ –µ—Å–ª–∏ —Ö–≤–∞—Ç–∞–µ—Ç –¥–∞–Ω–Ω—ã—Ö)
    MIN_TRAIN_FOR_ROLLING = 400  # –º—è–≥–∫–∏–π –ø–æ—Ä–æ–≥, —á—Ç–æ–±—ã –Ω–µ —Ç—Ä–∞—Ç–∏—Ç—å –≤—Ä–µ–º—è –Ω–∞ –∫—Ä–æ—à–µ—á–Ω—ã—Ö –≤—ã–±–æ—Ä–∫–∞—Ö
    if use_rolling_cv and len(X_train) >= MIN_TRAIN_FOR_ROLLING:
        scores = rolling_cross_validation(X_train, y_train, best_params, n_splits=5)
        logger.info("üìà Rolling CV F1_macro: %.4f", float(np.mean(scores)))
    elif use_rolling_cv:
        logger.warning("üöß Rolling CV –ø—Ä–æ–ø—É—â–µ–Ω (train=%d < %d)", len(X_train), MIN_TRAIN_FOR_ROLLING)

    # 4) —Ñ–∏–Ω–∞–ª—å–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ (model_trainer —Å–∞–º —Å–æ—Ö—Ä–∞–Ω–∏—Ç –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –≤ models/)
    train_final_model(X_train, y_train, best_params)

    # 5) –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –∞—Ä—Ö–∏–≤–∏—Ä—É–µ–º —Å–≤–µ–∂–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç—ã –ø–æ–¥ —Å–∏–º–≤–æ–ª/—Ç–∞–π–º—Å—Ç–µ–º–ø
    run_dir = _archive_artifacts(symbol, ts)

    # 6) –∑–∞–≥—Ä—É–∂–∞–µ–º —Å–≤–µ–∂—É—é –º–æ–¥–µ–ª—å –¥–ª—è –æ—Ü–µ–Ω–∫–∏ (fail-safe)
    model = cb.CatBoostClassifier()
    model_path = MODEL_DIR / "saved_model.cbm"
    try:
        model.load_model(str(model_path))
    except Exception as e:
        logger.exception("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å: %s", model_path)
        raise
    # 7) –°–æ—Ö—Ä–∞–Ω—è–µ–º –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–≥–æ–Ω–µ
    save_metadata(symbol, ts, best_params)

    # === –ß–ï–°–¢–ù–ê–Ø –û–¶–ï–ù–ö–ê –ò –ö–ê–õ–ò–ë–†–û–í–ö–ê ===
    # –¥–µ–ª–∏–º —Ç–µ—Å—Ç –≤–æ –≤—Ä–µ–º–µ–Ω–∏: –ø–µ—Ä–≤—ã–µ 70% ‚Üí –º–µ—Ç—Ä–∏–∫–∏, –ø–æ—Å–ª–µ–¥–Ω–∏–µ 30% ‚Üí –∫–∞–ª–∏–±—Ä–æ–≤–∫–∞
    n_test = len(X_test)
    cut = max(1, int(n_test * 0.7))  # 70/30
    X_eval, y_eval = X_test.iloc[:cut], y_test.iloc[:cut]
    X_hold, y_hold = X_test.iloc[cut:], y_test.iloc[cut:]
    pct_eval = 100.0 * len(X_eval) / max(1, n_test)
    pct_hold = 100.0 * len(X_hold) / max(1, n_test)
    logger.info("[Eval/Holdout] sizes: eval=%d (%.1f%%), holdout=%d (%.1f%%), total=%d",
                len(X_eval), pct_eval, len(X_hold), pct_hold, n_test)

    # –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞ —Ä–∞–∑–º–µ—Ä holdout –¥–ª—è –∏–∑–æ—Ç–æ–Ω–∏–∫–∏ (–∏–Ω–∞—á–µ –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –∫–∞–ª–∏–±—Ä–æ–≤–∫—É)
    MIN_HOLDOUT = 30
    if len(X_hold) >= MIN_HOLDOUT:
        calib_tuple = (X_hold, y_hold)
    else:
        logger.warning("Holdout —Å–ª–∏—à–∫–æ–º –º–∞–ª –¥–ª—è –∫–∞–ª–∏–±—Ä–æ–≤–∫–∏ (len=%d < %d) ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é –∫–∞–ª–∏–±—Ä–∞—Ç–æ—Ä",
                       len(X_hold), MIN_HOLDOUT)
        calib_tuple = None

    # –≤–∞–∂–Ω–æ: symbol ¬´–∫–∞–∫ –µ—Å—Ç—å¬ª, ts –ø–µ—Ä–µ–¥–∞—ë–º –æ—Ç–¥–µ–ª—å–Ω—ã–º –∞—Ä–≥—É–º–µ–Ω—Ç–æ–º (–±–µ–∑ –¥—É–±–ª–∏—Ä–æ–≤–∞–Ω–∏—è)
    evaluate_model(model, X_eval, y_eval, symbol=symbol, ts=ts, calib=calib_tuple)


if __name__ == "__main__":
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Bybit –∫–ª–∏–µ–Ω—Ç–∞ (–¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ OHLCV)
    client = HTTP(api_key=BYBIT_API_KEY, api_secret=BYBIT_API_SECRET)
    set_client(client)

    # –í—Ä–µ–º–µ–Ω–Ω–æ —Å—Ç–∞—Ç–∏—á–µ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ ‚Äî –ø–æ–∑–∂–µ –∑–∞–º–µ–Ω–∏–º –Ω–∞ top_pairs –∏–∑ pair_discovery
    symbols = ["BTCUSDT", "ETHUSDT"]

    for sym in symbols:
        train_on_symbol(sym)
