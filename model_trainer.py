import os
import pickle
import logging
from typing import Tuple, List

import numpy as np
import pandas as pd

from bayes_opt import BayesianOptimization
from sklearn.model_selection import TimeSeriesSplit, train_test_split
from sklearn.metrics import (
    f1_score,
    accuracy_score,
    precision_score,
    recall_score,
    ConfusionMatrixDisplay,
    classification_report,
)
from sklearn.preprocessing import StandardScaler
from sklearn.utils.class_weight import compute_sample_weight
from sklearn.cluster import KMeans
import catboost as cb
import matplotlib.pyplot as plt
import json

# ================= Project-specific imports =================
from config import (
    USE_RESAMPLING,
    RESAMPLING_STRATEGY,
    USE_CLASS_WEIGHTS,
    CLASS_WEIGHT_MODE,
    CONFIDENCE_THRESHOLDS,
)
from data_loader import get_processed_ohlcv
from feature_engineering import generate_target, select_features, generate_clustering

# ================= Optional: imblearn fallbacks =================
try:
    from imblearn.over_sampling import SMOTE
    from imblearn.under_sampling import RandomUnderSampler
except ImportError:
    SMOTE = None
    RandomUnderSampler = None

logger = logging.getLogger(__name__)


UP_THRESHOLD = 0.002
DOWN_THRESHOLD = 0.0015


def _sanitize_categoricals(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:
    """
    –ü—Ä–∏–≤–æ–¥–∏—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –∫ —Å—Ç—Ä–æ–∫–∞–º –∏ –∑–∞–ø–æ–ª–Ω—è–µ—Ç NaN, —á—Ç–æ–±—ã CatBoost –Ω–µ –ø–∞–¥–∞–ª.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –∫–æ–ø–∏—é df –∏ —Å–ø–∏—Å–æ–∫ cat-–∫–æ–ª–æ–Ω–æ–∫.
    """
    out = df.copy()
    cat_cols = out.select_dtypes(include=["object", "category", "string"]).columns.tolist()
    for c in cat_cols:
        out[c] = out[c].astype("string").fillna("__NA__")
    return out, cat_cols


def prepare_data(symbol: str, interval: str, threshold: float) -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–µ OHLCV, –≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Ñ–∏—á–∏/–∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—é,
    –¥–µ–ª–∏—Ç –Ω–∞ train/test –±–µ–∑ –ø–µ—Ä–µ–º–µ—à–∏–≤–∞–Ω–∏—è, –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ —Ä–µ—Å–µ–º–ø–ª–∏—Ç –¢–û–õ–¨–ö–û train,
    –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ—Ç —á–∏—Å–ª–æ–≤—ã–µ —Ñ–∏—á–∏, —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç scaler –≤ models/scaler.pkl.

    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: X_train_scaled, X_test_scaled, y_train, y_test
    """
    # 1) –¥–∞–Ω–Ω—ã–µ -> –∫–ª–∞—Å—Ç–µ—Ä—ã -> —Ñ–∏—á–∏/—Ç–∞—Ä–≥–µ—Ç
    df = get_processed_ohlcv(symbol, interval)
    df = generate_clustering(df)

    X, y = select_features(df)
    logger.info("Target distribution: %s", y.value_counts(normalize=True).to_dict())

    # 2) —Å–ø–ª–∏—Ç –ø–æ –≤—Ä–µ–º–µ–Ω–∏ (–±–µ–∑ shuffle)
    X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=False, test_size=0.2)
    # üîç –õ–æ–≥ –¥–∏—Å—Ç—Ä–∏–±—É—Ü–∏–∏ —Ç–∞—Ä–≥–µ—Ç–∞ –¥–ª—è –∫–æ–Ω—Ç—Ä–æ–ª—è —Å–º–µ—â–µ–Ω–∏—è
    logger.info("Train target dist: %s", y_train.value_counts(normalize=True).round(3).to_dict())
    logger.info("Test  target dist: %s", y_test.value_counts(normalize=True).round(3).to_dict())

    # --- –ì–ê–†–î: –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—ã —Å—Ä–∞–∑—É –ø–æ—Å–ª–µ —Å–ø–ª–∏—Ç–∞ (–Ω–∞ –≤—Å—è–∫–∏–π —Å–ª—É—á–∞–π) ---
    if len(X_train) != len(y_train):
        n_safe = min(len(X_train), len(y_train))
        logger.warning("[prepare_data] Train length mismatch: X=%d, y=%d ‚Üí aligning to %d",
                       len(X_train), len(y_train), n_safe)
        X_train = X_train.iloc[:n_safe].copy().reset_index(drop=True)
        y_train = y_train.iloc[:n_safe].copy().reset_index(drop=True)
    if len(X_test) != len(y_test):
        n_safe = min(len(X_test), len(y_test))
        logger.warning("[prepare_data] Test length mismatch: X=%d, y=%d ‚Üí aligning to %d",
                       len(X_test), len(y_test), n_safe)
        X_test = X_test.iloc[:n_safe].copy().reset_index(drop=True)
        y_test = y_test.iloc[:n_safe].copy().reset_index(drop=True)

    # 3) —Ä–∞–∑–±–∏–µ–Ω–∏–µ –ø–æ —Ç–∏–ø–∞–º
    cat_cols = X_train.select_dtypes(include=["object", "category"]).columns.tolist()
    num_cols = X_train.select_dtypes(include=["number"]).columns.tolist()
    if len(num_cols) == 0:
        raise ValueError("–ù–µ—Ç —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è —Å–∫–µ–π–ª–∏–Ω–≥–∞ ‚Äî –ø—Ä–æ–≤–µ—Ä—å select_features().")

    # --- –°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è: —É–±–∏—Ä–∞–µ–º/–∑–∞–ø–æ–ª–Ω—è–µ–º NaN —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω–æ –¥–ª—è X –∏ y ---
    # (–∞) —á–∏—Å–ª–æ–≤—ã–µ NaN ‚Äî –≤—ã–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—Ç—Ä–æ–∫–∏ (—á—Ç–æ–±—ã –Ω–µ –ø–æ—Ä—Ç–∏—Ç—å —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è)
    mask_num_train = X_train[num_cols].notna().all(axis=1)
    mask_num_test  = X_test[num_cols].notna().all(axis=1)

    # (–±) —Ç–∞—Ä–≥–µ—Ç –±–µ–∑ NaN
    mask_y_train = y_train.notna()
    mask_y_test  = y_test.notna()

    # (–≤) –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ NaN –¥–ª—è CatBoost ‚Äî –ó–ê–ü–û–õ–ù–Ø–ï–ú —Å—Ç—Ä–æ–∫–æ–π "__NA__" –∏ –ø—Ä–∏–≤–æ–¥–∏–º –∫ str
    if cat_cols:
        X_train.loc[:, cat_cols] = X_train[cat_cols].astype("string").fillna("__NA__")
        X_test.loc[:, cat_cols]  = X_test[cat_cols].astype("string").fillna("__NA__")
        mask_cat_train = pd.Series(True, index=X_train.index)  # —É–∂–µ –∑–∞–ø–æ–ª–Ω–µ–Ω–æ
        mask_cat_test  = pd.Series(True, index=X_test.index)
    else:
        mask_cat_train = pd.Series(True, index=X_train.index)
        mask_cat_test  = pd.Series(True, index=X_test.index)

    # –ø—Ä–∏–º–µ–Ω—è–µ–º —Å–æ–≥–ª–∞—Å–æ–≤–∞–Ω–Ω—ã–µ –º–∞—Å–∫–∏
    keep_train = (mask_num_train & mask_cat_train & mask_y_train)
    keep_test  = (mask_num_test  & mask_cat_test  & mask_y_test)

    if keep_train.sum() < len(keep_train):
        logger.warning("[prepare_data] Dropping %d rows with NaN (train)", int((~keep_train).sum()))
    if keep_test.sum() < len(keep_test):
        logger.warning("[prepare_data] Dropping %d rows with NaN (test)", int((~keep_test).sum()))

    X_train = X_train.loc[keep_train].reset_index(drop=True)
    y_train = y_train.loc[keep_train].reset_index(drop=True).astype(int)

    X_test = X_test.loc[keep_test].reset_index(drop=True)
    y_test = y_test.loc[keep_test].reset_index(drop=True).astype(int)

    # 4) –æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π —Ä–µ—Å–µ–º–ø–ª–∏–Ω–≥ ‚Äî –¢–û–õ–¨–ö–û –Ω–∞ train
    if USE_RESAMPLING and RESAMPLING_STRATEGY != "none":
        if RESAMPLING_STRATEGY == "smote":
            if SMOTE is None:
                logger.warning("[Resampling] imblearn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é SMOTE.")
            else:
                logger.info("[Resampling] Applying SMOTE on train set")
                X_train_num = X_train[num_cols].reset_index(drop=True)
                X_train_cat = X_train[cat_cols].reset_index(drop=True) if cat_cols else pd.DataFrame(index=X_train_num.index)

                smote = SMOTE(random_state=42)
                X_num_res, y_train_res = smote.fit_resample(X_train_num, y_train.reset_index(drop=True))

                if not X_train_cat.empty:
                    rep = max(1, int(np.ceil(len(y_train_res) / max(1, len(X_train_cat)))))
                    X_cat_rep = pd.concat([X_train_cat] * rep, ignore_index=True).iloc[:len(y_train_res)]
                else:
                    X_cat_rep = pd.DataFrame(index=np.arange(len(y_train_res)))

                X_train = pd.concat(
                    [pd.DataFrame(X_num_res, columns=num_cols), X_cat_rep.reset_index(drop=True)],
                    axis=1
                )
                y_train = y_train_res.reset_index(drop=True).astype(int)

                # –ø–æ—Å–ª–µ SMOTE ‚Äî —É–±–µ–¥–∏–º—Å—è, —á—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –æ—Å—Ç–∞–ª–∏—Å—å string
                if cat_cols:
                    for c in cat_cols:
                        if c in X_train.columns:
                            X_train[c] = X_train[c].astype("string")
        elif RESAMPLING_STRATEGY == "undersample":
            if RandomUnderSampler is None:
                logger.warning("[Resampling] imblearn –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é undersample.")
            else:
                logger.info("[Resampling] Applying RandomUnderSampler on train set")
                X_train, y_train = RandomUnderSampler(random_state=42).fit_resample(
                    X_train.reset_index(drop=True), y_train.reset_index(drop=True)
                )
                y_train = y_train.astype(int)
        else:
            logger.info("[Resampling] strategy=none ‚Äî –ø—Ä–æ–ø—É—Å–∫–∞—é")
    else:
        logger.info("[Resampling] –æ—Ç–∫–ª—é—á–µ–Ω ‚Äî –∏—Å–ø–æ–ª—å–∑—É–µ–º class weights –Ω–∞ —ç—Ç–∞–ø–µ –æ–±—É—á–µ–Ω–∏—è.")

    # 5) –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º —á–∏—Å–ª–æ–≤—ã–µ
    scaler = StandardScaler()
    X_train_num_scaled = pd.DataFrame(
        scaler.fit_transform(X_train[num_cols]),
        columns=num_cols, index=X_train.index
    )
    X_test_num_scaled = pd.DataFrame(
        scaler.transform(X_test[num_cols]),
        columns=num_cols, index=X_test.index
    )

    # 6) —Å–æ–±–∏—Ä–∞–µ–º –æ–±—Ä–∞—Ç–Ω–æ —Å –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º–∏ (–µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å)
    if cat_cols:
        # –≥–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º, —á—Ç–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ‚Äî string –∏ –±–µ–∑ NaN
        X_train_cat = X_train[cat_cols].astype("string").fillna("__NA__").reset_index(drop=True)
        X_test_cat  = X_test[cat_cols].astype("string").fillna("__NA__").reset_index(drop=True)

        X_train_scaled = pd.concat([X_train_num_scaled.reset_index(drop=True), X_train_cat], axis=1)
        X_test_scaled  = pd.concat([X_test_num_scaled.reset_index(drop=True),  X_test_cat],  axis=1)
    else:
        X_train_scaled = X_train_num_scaled.reset_index(drop=True)
        X_test_scaled  = X_test_num_scaled.reset_index(drop=True)

    # 7) —Å–æ—Ö—Ä–∞–Ω—è–µ–º scaler –≤ PROJECT_ROOT/models/scaler.pkl
    project_root = os.path.dirname(os.path.abspath(__file__))  # model_trainer.py
    project_root = os.path.dirname(project_root)               # –ø–æ–¥–Ω—è—Ç—å—Å—è –Ω–∞ —É—Ä–æ–≤–µ–Ω—å –ø—Ä–æ–µ–∫—Ç–∞
    models_dir = os.path.join(project_root, "models")
    os.makedirs(models_dir, exist_ok=True)
    with open(os.path.join(models_dir, "scaler.pkl"), "wb") as f:
        pickle.dump(scaler, f)

    return (
        X_train_scaled.reset_index(drop=True),
        X_test_scaled.reset_index(drop=True),
        y_train.reset_index(drop=True),
        y_test.reset_index(drop=True),
    )




def optimize_catboost(X_train: pd.DataFrame, y_train: pd.Series) -> dict:
    """
            –ë–∞–π–µ—Å-–æ–ø—Ç –ø–æ CatBoost —Å –ß–ï–°–¢–ù–û–ô –≤—Ä–µ–º–µ–Ω–Ω–æ–π –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π (Purged CV + Embargo).
            –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —Ñ–æ–ª–¥—ã, –≥–¥–µ train/val —Å–ª–∏—à–∫–æ–º –º–∞–ª—ã ‚Äî —á—Ç–æ–±—ã –Ω–µ —É—á–∏—Ç—å—Å—è –Ω–∞ —à—É–º–µ.
            –í–æ–∑–≤—Ä–∞—â–∞–µ–º –ª—É—á—à–∏–π –Ω–∞–±–æ—Ä –≥–∏–ø–µ—Ä–æ–≤ –ø–æ —Å—Ä–µ–¥–Ω–µ–º—É F1_macro –Ω–∞ –≤–∞–ª. —Ñ–æ–ª–¥–∞—Ö.
            """
    from cv_utils import purged_cv_splits
    from config import EMBARGO_BARS, MIN_CV_TRAIN, MIN_CV_VAL, N_SPLITS_BO
    from sklearn.utils.class_weight import compute_sample_weight
    # ---- Prep before BayesOpt: –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—ã –∏ —á–∏–Ω–∏–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ ----

    # —Å–∞–Ω–∏—Ç–∏–∑–∏—Ä—É–µ–º –æ–¥–∏–Ω —Ä–∞–∑ –≤–µ—Å—å train:
    X_train_sanitized, cat_cols = _sanitize_categoricals(X_train)

    # –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—ã X –∏ y (–µ—Å–ª–∏ –≤–¥—Ä—É–≥ —Ä–∞–∑–æ—à–ª–∏—Å—å)
    if len(X_train_sanitized) != len(y_train):
        nX, nY = len(X_train_sanitized), len(y_train)
        logger.warning("[BayesOpt] Length mismatch before CV: X_train=%d, y_train=%d. Aligning to min %d.",
                       nX, nY, min(nX, nY))
        n_safe = min(nX, nY)
        X_train_sanitized = X_train_sanitized.iloc[:n_safe].reset_index(drop=True)
        y_train = y_train.iloc[:n_safe].reset_index(drop=True)

    n = len(X_train_sanitized)  # —Ä–∞–∑–º–µ—Ä, —Å –∫–æ—Ç–æ—Ä—ã–º —Ä–∞–±–æ—Ç–∞–µ–º –¥–∞–ª—å—à–µ

    def evaluate(depth, learning_rate, l2_leaf_reg, bagging_temperature,
                 random_strength, rsm):
        params = {
            "iterations": 600,  # –±–æ–ª—å—à–µ –∏—Ç–µ—Ä–∞—Ü–∏–π, –±—É–¥–µ—Ç early stop
            "depth": int(depth),
            "learning_rate": float(learning_rate),
            "l2_leaf_reg": float(l2_leaf_reg),
            "bagging_temperature": float(bagging_temperature),
            "random_strength": float(random_strength),
            "rsm": float(rsm),  # feature subsampling
            "bootstrap_type": "Bayesian",
            "loss_function": "MultiClass",
            "verbose": False,
            "random_seed": 42,
        }

        scores = []
        used_folds = 0

        for fold_id, (tr_idx, val_idx) in enumerate(
                purged_cv_splits(n=n, n_splits=N_SPLITS_BO, embargo=EMBARGO_BARS), start=1
        ):
            tr_len, val_len = len(tr_idx), len(val_idx)
            if tr_len < MIN_CV_TRAIN or val_len < MIN_CV_VAL:
                logger.info(
                    "[BayesOpt][fold %d] skipped: train=%d (min %d), val=%d (min %d), embargo=%d",
                    fold_id, tr_len, MIN_CV_TRAIN, val_len, MIN_CV_VAL, EMBARGO_BARS
                )
                continue

            X_tr, X_val = X_train_sanitized.iloc[tr_idx], X_train_sanitized.iloc[val_idx]
            y_tr, y_val = y_train.iloc[tr_idx], y_train.iloc[val_idx]

            sw = compute_sample_weight(class_weight="balanced", y=y_tr)

            model = cb.CatBoostClassifier(**params)
            try:
                model.fit(
                    X_tr, y_tr,
                    eval_set=(X_val, y_val),
                    sample_weight=sw,
                    cat_features=(cat_cols or None),
                    early_stopping_rounds=40,
                    verbose=False
                )
                preds = np.asarray(model.predict(X_val)).ravel().astype(int)
                score = f1_score(y_val, preds, average="macro", zero_division=0)
            except Exception as e:
                logger.warning("[BayesOpt][fold %d] failed: %s", fold_id, e)
                score = 0.0

            scores.append(float(score))
            used_folds += 1

        if not scores:
            logger.warning("[BayesOpt] No valid folds ‚Äî returning 0.0 for this point")
            return 0.0

        mean_score = float(np.mean(scores))
        logger.debug("[BayesOpt] used_folds=%d | mean F1_macro=%.4f", used_folds, mean_score)
        return mean_score

    optimizer = BayesianOptimization(
        f=evaluate,
        pbounds={
            "depth": (5, 7),  # —á—É—Ç—å –≥–ª—É–±–∂–µ
            "learning_rate": (0.05, 0.12),  # —Å—Ä–µ–¥–Ω—è—è –∑–æ–Ω–∞
            "l2_leaf_reg": (4.0, 15.0),  # –ø–æ–º—è–≥—á–µ
            "bagging_temperature": (0.1, 0.8),
            "random_strength": (0.5, 6.0),  # —Å–Ω–∏–∑–∏–ª–∏ –≤–µ—Ä—Ö
            "rsm": (0.70, 0.95),  # –Ω–µ —Ç–∞–∫ –Ω–∏–∑–∫–æ
        },
        random_state=42,
        verbose=0,
    )

    optimizer.maximize(init_points=3, n_iter=7)

    best = optimizer.max["params"]
    best = {
        "depth": int(round(best["depth"])),
        "learning_rate": float(best["learning_rate"]),
        "l2_leaf_reg": float(best["l2_leaf_reg"]),
        "bagging_temperature": float(best["bagging_temperature"]),
        "random_strength": float(best["random_strength"]),
        "rsm": float(best["rsm"]),
    }
    logger.info("üîß Best hyperparams (purged CV): %s", best)
    return best


def rolling_cross_validation(
    X: pd.DataFrame,
    y: pd.Series,
    model_params: dict,
    n_splits: int = 5
):
    # --- –ì–ê–†–î: –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏–µ –¥–ª–∏–Ω X / y ---
    nX, nY = len(X), len(y)
    if nX != nY:
        logger.warning("[RollingCV] Length mismatch: X=%d, y=%d. Aligning to min length.", nX, nY)
        n_safe = min(nX, nY)
        X = X.iloc[:n_safe].reset_index(drop=True)
        y = y.iloc[:n_safe].reset_index(drop=True)

    # --- –°–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –Ω–∞ –≤–µ—Å—å X: str + fillna("__NA__") ---

    X, cat_cols = _sanitize_categoricals(X)

    """
    Walk-forward sanity CV —Å —ç–º–±–∞—Ä–≥–æ –∏ –≥–∞—Ä–¥–æ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã—Ö —Ä–∞–∑–º–µ—Ä–æ–≤.
    - –ë–µ–∑ —Ä–µ—Å–µ–º–ø–ª–∏–Ω–≥–∞; –±–∞–ª–∞–Ω—Å ‚Äî —á–µ—Ä–µ–∑ class_weight (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ).
    - –≠–º–±–∞—Ä–≥–æ ¬´–≤—ã—Ä–µ–∑–∞–µ—Ç¬ª –∑–∞–∑–æ—Ä –º–µ–∂–¥—É train –∏ test (–ø–æ –≤—Ä–µ–º–µ–Ω–∏).
    """
    from config import EMBARGO_BARS_ROLLING, MIN_ROLL_TRAIN, MIN_ROLL_TEST

    logger.info("Rolling CV started")
    scores: list[float] = []

    total_len = len(X)
    if total_len < (MIN_ROLL_TRAIN + MIN_ROLL_TEST + EMBARGO_BARS_ROLLING):
        logger.warning(
            "Dataset too small for rolling CV: len=%d (need >= %d)",
            total_len, MIN_ROLL_TRAIN + MIN_ROLL_TEST + EMBARGO_BARS_ROLLING
        )
        return scores

    # –±–∞–∑–æ–≤–æ–µ –æ–∫–Ω–æ: ~60% train, ~20% test, ~10% —à–∞–≥ ‚Äî –∫–∞–∫ —Ä–∞–Ω—å—à–µ
    window_size = int(total_len * 0.6)
    test_size   = int(total_len * 0.2)
    step        = max(1, int(total_len * 0.1))

    # –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏
    params = model_params.copy()
    params["depth"] = int(params.get("depth", 6))
    params.update({
        "loss_function": "MultiClass",
        "random_seed": 42,
        "iterations": 300,
        "verbose": False,
    })

    for i in range(n_splits):
        train_start = i * step
        train_end   = train_start + window_size
        gap_start   = train_end
        gap_end     = gap_start + EMBARGO_BARS_ROLLING
        test_start  = gap_end
        test_end    = test_start + test_size

        if test_end > total_len:
            break

        # –≥–∞—Ä–¥—ã –Ω–∞ —Ä–∞–∑–º–µ—Ä—ã
        tr_len = train_end - train_start
        te_len = test_end - test_start
        if tr_len < MIN_ROLL_TRAIN or te_len < MIN_ROLL_TEST:
            logger.info(
                "Fold %d skipped: train=%d (min %d), test=%d (min %d), embargo=%d",
                i + 1, tr_len, MIN_ROLL_TRAIN, te_len, MIN_ROLL_TEST, EMBARGO_BARS_ROLLING
            )
            continue

        X_train_raw = X.iloc[train_start:train_end]
        y_train_ = y.iloc[train_start:train_end]
        X_test_raw = X.iloc[test_start:test_end]
        y_test_ = y.iloc[test_start:test_end]

        # —Å–∞–Ω–∏—Ç–∏–∑–∏—Ä—É–µ–º –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä—è–º–æ –Ω–∞ —Å—Ä–µ–∑–∞—Ö
        X_train_s, cat_cols = _sanitize_categoricals(X_train_raw)
        X_test_s, _ = _sanitize_categoricals(X_test_raw)

        # –≤–µ—Å–∞ –∫–ª–∞—Å—Å–æ–≤ (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω—ã)
        sample_weight = None
        try:
            from config import USE_CLASS_WEIGHTS, CLASS_WEIGHT_MODE
            if USE_CLASS_WEIGHTS and CLASS_WEIGHT_MODE == "balanced":
                sample_weight = compute_sample_weight(class_weight="balanced", y=y_train_)
        except Exception:
            pass

        model = cb.CatBoostClassifier(**params)
        model.fit(
            X_train_s, y_train_,
            cat_features=(cat_cols or None),
            sample_weight=sample_weight,
            verbose=False
        )

        y_hat = np.asarray(model.predict(X_test_s)).ravel().astype(int)
        score = f1_score(y_test_, y_hat, average="macro", zero_division=0)
        logger.info(
            "Fold %d: train=%d, gap=%d, test=%d | F1_macro=%.4f",
            i + 1, tr_len, EMBARGO_BARS_ROLLING, te_len, score
        )
        scores.append(score)

    if scores:
        logger.info("Rolling CV complete. Mean F1_macro: %.4f", float(np.mean(scores)))
    else:
        logger.warning("Rolling CV produced no valid folds ‚Äî check sizes/embargo")

    return scores


def train_final_model(X_train: pd.DataFrame, y_train: pd.Series, best_params: dict):
    """
    –§–∏–Ω–∞–ª—å–Ω—ã–π —Ñ–∏—Ç CatBoost:
    - —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö (str + fillna('__NA__'))
    - class weights (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –≤ config)
    - early stopping –Ω–∞ train (eval_set=train) ‚Äî –æ—Å–æ–∑–Ω–∞–Ω–Ω–æ
    - —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏, cat_features –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—á–µ—Å–∫–∏—Ö –æ—Ç—á—ë—Ç–æ–≤
    """
    from sklearn.utils.class_weight import compute_sample_weight

    os.makedirs("models", exist_ok=True)
    os.makedirs("outputs", exist_ok=True)

    # --- –ì–ê–†–î: –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–µ–º –¥–ª–∏–Ω—ã X/y (–µ—Å–ª–∏ –∫—Ç–æ-—Ç–æ —Ç—Ä–æ–Ω—É–ª y –ø–æ –¥–æ—Ä–æ–≥–µ) ---
    if len(X_train) != len(y_train):
        n_safe = min(len(X_train), len(y_train))
        logger.warning("[FinalFit] Length mismatch: X=%d, y=%d ‚Üí aligning to %d",
                       len(X_train), len(y_train), n_safe)
        X_train = X_train.iloc[:n_safe].reset_index(drop=True)
        y_train = y_train.iloc[:n_safe].reset_index(drop=True)

    # --- —Ç–∏–ø y: —Å—Ç—Ä–æ–≥–æ int ---
    y_train = y_train.astype(int).reset_index(drop=True)

    # --- —Å–∞–Ω–∏—Ç–∏–∑–∞—Ü–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö + —Å–ø–∏—Å–æ–∫ cat_features ---
    X_train_s, cat_features = _sanitize_categoricals(X_train)
    # CatBoost –Ω–æ—Ä–º–∞–ª—å–Ω–æ –ø—Ä–∏–Ω–∏–º–∞–µ—Ç None –≤–º–µ—Å—Ç–æ –ø—É—Å—Ç–æ–≥–æ —Å–ø–∏—Å–∫–∞
    cat_features_arg = cat_features or None

    # --- (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ª—ë–≥–∫–∏–π feature bagging: —Å–ª—É—á–∞–π–Ω–æ ¬´—É—Ä–µ–∑–∞—Ç—å¬ª —Ñ–∏—á–∏ –Ω–∞ 5% ---
    try:
        from config import FEATURE_BAGGING_FRAC
    except Exception:
        FEATURE_BAGGING_FRAC = None

    if FEATURE_BAGGING_FRAC and 0.0 < FEATURE_BAGGING_FRAC < 1.0:
        feat_cols = X_train_s.columns.tolist()
        # –Ω–µ –∏–º–µ–µ—Ç —Å–º—ã—Å–ª–∞ –Ω–∞ –æ—á–µ–Ω—å –º–∞–ª–µ–Ω—å–∫–æ–º —á–∏—Å–ª–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
        if len(feat_cols) >= 20:
            keep_n = max(1, int(len(feat_cols) * float(FEATURE_BAGGING_FRAC)))
            rng = np.random.default_rng(42)  # —Ñ–∏–∫—Å–∏—Ä—É–µ–º –æ—Ç–±–æ—Ä –∫–æ–ª–æ–Ω–æ–∫
            keep_cols = sorted(rng.choice(feat_cols, size=keep_n, replace=False).tolist())

            # —Å—É–∂–∞–µ–º –æ–±—É—á–∞—é—â—É—é –º–∞—Ç—Ä–∏—Ü—É –∏ —Å–ø–∏—Å–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö
            X_train_s = X_train_s[keep_cols]
            if cat_features:
                cat_features = [c for c in cat_features if c in keep_cols]
            cat_features_arg = (cat_features or None)

            logger.info("[FeatureBagging] kept %d/%d features", keep_n, len(feat_cols))

            # üîí –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫, —á—Ç–æ–±—ã –∏–Ω—Ñ–µ—Ä–µ–Ω—Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–ª —Ç–æ—á–Ω–æ —Ç–æ—Ç –∂–µ –ø–æ—Ä—è–¥–æ–∫/–Ω–∞–±–æ—Ä
            os.makedirs("models", exist_ok=True)
            with open("models/feature_columns.pkl", "wb") as f:
                pickle.dump(keep_cols, f)
        else:
            logger.info("[FeatureBagging] skipped (features=%d < 20)", len(feat_cols))

    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π –ø–æ—Ä—è–¥–æ–∫/–Ω–∞–±–æ—Ä —Ñ–∏—á (–¥–ª—è —Å—Ç—Ä–æ–≥–æ–≥–æ –≤—ã—Ä–∞–≤–Ω–∏–≤–∞–Ω–∏—è –ø—Ä–∏ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–µ)
    feature_columns = X_train_s.columns.tolist()
    try:
        with open(os.path.join("models", "feature_columns.pkl"), "wb") as f:
            pickle.dump(feature_columns, f)
        # (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ) –ø–æ-—Å–∏–º–≤–æ–ª—å–Ω–æ:
        # with open(os.path.join("models", f"feature_columns_{symbol}.pkl"), "wb") as f:
        #     pickle.dump(feature_columns, f)
        logger.info("[Artifacts] Saved feature_columns.pkl (%d cols)", len(feature_columns))
    except Exception as e:
        logger.warning("Failed to save feature_columns.pkl: %s", e)

    # --- –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –º–æ–¥–µ–ª–∏ ---
    params = best_params.copy()
    params["depth"] = int(params.get("depth", 6))
    params.update({
        "loss_function": "MultiClass",
        "random_seed": 42,
        "iterations": 500,
        "verbose": False,
    })

    # --- class weights (–µ—Å–ª–∏ –≤–∫–ª—é—á–µ–Ω–æ –≤ config) ---
    sample_weight = None
    try:
        from config import USE_CLASS_WEIGHTS, CLASS_WEIGHT_MODE
        if USE_CLASS_WEIGHTS and CLASS_WEIGHT_MODE == "balanced":
            sample_weight = compute_sample_weight(class_weight="balanced", y=y_train)
            logger.info("[ClassWeight] Enabled ‚Äî balanced per-sample weights applied")
    except Exception:
        pass

    # --- –æ–±—É—á–µ–Ω–∏–µ —Å —Ä–∞–Ω–Ω–∏–º —Å—Ç–æ–ø–æ–º –ø–æ train (–∫–∞–∫ –¥–æ–≥–æ–≤–æ—Ä–∏–ª–∏—Å—å) ---
    # --- –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –≤–∞–ª–∏–¥: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10% —Ç—Ä–µ–π–Ω–∞ ---
    # --- –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–π –≤–∞–ª–∏–¥: –ø–æ—Å–ª–µ–¥–Ω–∏–µ 10% —Ç—Ä–µ–π–Ω–∞ ---
    n = len(X_train_s)
    cut = max(1, int(n * 0.9))
    X_tr, y_tr = X_train_s.iloc[:cut], y_train.iloc[:cut]
    X_val, y_val = X_train_s.iloc[cut:], y_train.iloc[cut:]

    # --- –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –Ω–∞—Ä–µ–∑–∞–µ–º –≤–µ—Å–∞ –ø–æ–¥ train-—á–∞—Å—Ç—å ---
    sw_tr = None
    if 'sample_weight' in locals() and sample_weight is not None:
        # –ø—Ä–∏–≤–æ–¥–∏–º –∫ numpy –∏ –Ω–∞—Ä–µ–∑–∞–µ–º –ø–æ cut, –µ—Å–ª–∏ –¥–ª–∏–Ω—ã —Å–æ–≤–ø–∞–¥–∞—é—Ç
        sw_arr = np.asarray(sample_weight)
        if len(sw_arr) == len(y_train):
            sw_tr = sw_arr[:cut]
        else:
            # –Ω–∞ –≤—Å—è–∫–∏–π: –µ—Å–ª–∏ –≤–¥—Ä—É–≥ —É–∂–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç —Å train-—á–∞—Å—Ç—å—é
            sw_tr = sw_arr

    model = cb.CatBoostClassifier(**params)
    model.fit(
        X_tr, y_tr,
        sample_weight=sw_tr,
        cat_features=cat_features_arg,
        eval_set=(X_val, y_val),
        early_stopping_rounds=40,
        verbose=False,
    )

    # --- —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–æ–≤ ---
    model.save_model("models/saved_model.cbm")
    with open("models/cat_features.pkl", "wb") as f:
        pickle.dump(cat_features, f)  # —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ (–º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º)

    # --- –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∞ –Ω–∞ train ---
    y_pred = np.asarray(model.predict(X_train_s)).ravel().astype(int)
    proba  = np.asarray(model.predict_proba(X_train_s))
    conf   = proba.max(axis=1)

    f1  = f1_score(y_train, y_pred, average="macro")
    acc = accuracy_score(y_train, y_pred)
    prec = precision_score(y_train, y_pred, average=None, labels=[0, 1, 2], zero_division=0)
    rec  = recall_score(y_train, y_pred, average=None, labels=[0, 1, 2], zero_division=0)

    logger.info("Final model ‚Äî Accuracy: %.4f, F1_macro: %.4f", acc, f1)
    for i in range(3):
        logger.info("Class %d ‚Äî Precision: %.3f, Recall: %.3f", i, prec[i], rec[i])

    # Confusion matrix (train)
    ConfusionMatrixDisplay.from_predictions(y_train, y_pred, cmap="viridis")
    plt.title("Confusion Matrix (train)")
    plt.savefig("outputs/confusion_matrix.png")
    plt.close()

    # Feature importance (—É—Å—Ç–æ–π—á–∏–≤–æ –∫ –≤–µ—Ä—Å–∏—è–º CatBoost)
    importances = model.get_feature_importance(prettified=True)
    feat_col = "Feature Id" if "Feature Id" in importances.columns else (
        "Feature" if "Feature" in importances.columns else importances.columns[0]
    )
    val_col = "Importances" if "Importances" in importances.columns else importances.columns[-1]
    plt.figure(figsize=(10, 6))
    plt.barh(importances[feat_col], importances[val_col])
    plt.tight_layout()
    plt.savefig("outputs/catboost_feature_importance.png")
    plt.close()

    # –¢–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç—á—ë—Ç
    with open("outputs/classification_report.txt", "w") as f:
        f.write(classification_report(y_train, y_pred, target_names=["Down", "Up", "Neutral"], zero_division=0))

    # –ú–µ—Ç—Ä–∏–∫–∏ –Ω–∞ ‚Äú—É–≤–µ—Ä–µ–Ω–Ω—ã—Ö‚Äù –ø–æ–¥–º–Ω–æ–∂–µ—Å—Ç–≤–∞—Ö
    for th in CONFIDENCE_THRESHOLDS:
        idx = conf >= th
        cov = float(np.mean(idx)) if len(idx) else 0.0
        if np.sum(idx) == 0:
            logger.warning("[Conf >= %.2f] Coverage: %.3f ‚Äî –Ω–µ—Ç —É–≤–µ—Ä–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π", th, cov)
            continue
        y_true_c = np.asarray(y_train)[idx]
        y_pred_c = y_pred[idx]
        acc_c = accuracy_score(y_true_c, y_pred_c)
        f1_c  = f1_score(y_true_c, y_pred_c, average="macro", zero_division=0)
        logger.info("[Conf >= %.2f] Coverage: %.3f | Acc: %.4f | F1_macro: %.4f", th, cov, acc_c, f1_c)

        ConfusionMatrixDisplay.from_predictions(y_true_c, y_pred_c, cmap="viridis")
        plt.title(f"Confusion Matrix @ Confidence ‚â• {th:.2f} (train)")
        plt.savefig(f"outputs/conf_matrix_conf_{int(th*100)}.png")
        plt.close()


def load_model_and_scaler(
    model_path="models/saved_model.cbm",
    scaler_path="models/scaler.pkl",
    cat_features_path="models/cat_features.pkl",
    feature_columns_path="models/feature_columns.pkl",
):
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç –º–æ–¥–µ–ª—å, —Å–∫–µ–π–ª–µ—Ä, —Å–ø–∏—Å–æ–∫ cat-—Ñ–∏—á –∏ (–µ—Å–ª–∏ –µ—Å—Ç—å) —Ñ–∞–∫—Ç–∏—á–µ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ –∫–æ–ª–æ–Ω–æ–∫,
    –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–Ω—ã–π –ø—Ä–∏ —Ñ–∏–Ω–∞–ª—å–Ω–æ–º —Ñ–∏—Ç–µ (feature bagging).
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç: model, scaler, cat_features, feature_columns (–∏–ª–∏ None).
    """
    import os
    import pickle
    import catboost as cb

    base_dir = os.path.dirname(os.path.abspath(__file__))
    model_path = os.path.join(base_dir, model_path)
    scaler_path = os.path.join(base_dir, scaler_path)
    cat_features_path = os.path.join(base_dir, cat_features_path)
    feature_columns_path = os.path.join(base_dir, feature_columns_path)

    if not os.path.exists(model_path):
        raise FileNotFoundError(f"‚ùå Model file not found: {model_path}")
    if not os.path.exists(scaler_path):
        raise FileNotFoundError(f"‚ùå Scaler file not found: {scaler_path}")
    if not os.path.exists(cat_features_path):
        raise FileNotFoundError(f"‚ùå Cat features file not found: {cat_features_path}")

    model = cb.CatBoostClassifier()
    model.load_model(model_path)

    with open(scaler_path, "rb") as f:
        scaler = pickle.load(f)
    with open(cat_features_path, "rb") as f:
        cat_features = pickle.load(f)

    feature_columns = None
    if os.path.exists(feature_columns_path):
        try:
            with open(feature_columns_path, "rb") as f:
                feature_columns = pickle.load(f)
        except Exception:
            feature_columns = None  # –Ω–µ –∫—Ä–∏—Ç–∏—á–Ω–æ

    print(f"[DEBUG] –ó–∞–≥—Ä—É–∂–µ–Ω–∞ –º–æ–¥–µ–ª—å: {os.path.basename(model_path)}")
    print(f"[DEBUG] –ó–∞–≥—Ä—É–∂–µ–Ω —Å–∫–µ–π–ª–µ—Ä: {os.path.basename(scaler_path)}")
    print(f"[DEBUG] –ó–∞–≥—Ä—É–∂–µ–Ω—ã cat_features ({len(cat_features)}): {cat_features}")
    if feature_columns is not None:
        print(f"[DEBUG] –ó–∞–≥—Ä—É–∂–µ–Ω—ã feature_columns ({len(feature_columns)})")

    return model, scaler, cat_features, feature_columns




def predict_on_batch(model, X_input, cat_features=None, feature_columns=None):
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç:
      preds: List[int]
      confidences: List[float] (max prob per row)
    –ï—Å–ª–∏ –ø–µ—Ä–µ–¥–∞–Ω feature_columns ‚Äî X_input –±—É–¥–µ—Ç –≤—ã—Ä–æ–≤–Ω–µ–Ω:
      - –¥–æ–±–∞–≤–∏–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –∫–æ–ª–æ–Ω–∫–∏ —Å NaN,
      - –æ—Ç—Å–æ—Ä—Ç–∏—Ä—É–µ–º –≤ –ø—Ä–∞–≤–∏–ª—å–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ.
    """
    import numpy as np
    import pandas as pd
    import catboost as cb

    X = X_input.copy()

    # 1) –í—ã—Ä–æ–≤–Ω—è—Ç—å –∫–æ–ª–æ–Ω–∫–∏ –ø–æ–¥ feature_columns (–µ—Å–ª–∏ –µ—Å—Ç—å)
    if feature_columns is not None:
        # –¥–æ–±–∞–≤–∏—Ç—å –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ
        missing = [c for c in feature_columns if c not in X.columns]
        for c in missing:
            X[c] = np.nan
        # –ª–∏—à–Ω–∏–µ –æ—Å—Ç–∞–≤–∏—Ç—å ‚Äî CatBoost –∏–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –Ω–µ –±—É–¥–µ—Ç, –ø–æ—ç—Ç–æ–º—É –æ—Ç—Ñ–∏–ª—å—Ç—Ä—É–µ–º
        X = X[feature_columns]

    # 2) –°–∞–Ω–∏—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ: str + fillna("__NA__")
    if cat_features:
        for c in cat_features:
            if c in X.columns:
                X[c] = X[c].astype("string").fillna("__NA__")

    # 3) CatBoost: cat_features –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –∏–Ω–¥–µ–∫—Å–∞–º–∏
    cat_idx = None
    if cat_features:
        cat_idx = [X.columns.get_loc(c) for c in cat_features if c in X.columns]

    pool = cb.Pool(X, cat_features=cat_idx)
    probs = model.predict_proba(pool)        # (n, C)
    preds = model.predict(pool)              # (n,) –∏–ª–∏ (n,1)
    preds = np.array(preds).astype(int).ravel().tolist()
    confs = np.max(probs, axis=1).tolist()
    return preds, confs
